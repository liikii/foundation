the purpose of computation is insight, not numbers. 
richard hamming
计算的目的不在于数字本身， 而在于洞察其背后的意义。 

https://www.seas.harvard.edu/courses/cs152/2021sp/resources.html
The Curry-Howard Isomorphism
Resources
Text books
A number of excellent books and on-line resources overlap with the course's content and can provide alternate explanations despite differences in notation and approach. Let the instructor know if you have trouble finding the intersection between these resources and the course content.

"Types and Programming Languages" by Benjamin C. Pierce, MIT Press, 2002.
Available on reserve at the library.
"Software Foundations" by Benjamin C. Pierce et al., Volume 1: Logical Foundations and Volume 2: Programming Language Foundations.
Available as literate Coq files.
Also available as literate Agda files thanks to Philip Wadler, Wen Kokke and Jeremy Siek.
"Practical Foundations for Programming Languages" by Robert Harper, Cambridge University Press, 2013.
Draft available on Harper's website.
"Concepts in Programming Languages" by John C. Mitchell, Cambridge University Press, 2003.
Available online through Harvard University Libraries eContent Collection.
"The Formal Semantics of Programming Languages" by Glynn Winskel, MIT Press, 1993.
Available on reserve at the library.
"Programming Languages: Application and Interpretation" by Shriram Krishnamurthi.
There are two editions, both available on the author's website: http://www.cs.brown.edu/~sk/Publications/Books/ProgLangs/.
OCaml resources
Installation: https://ocaml.org/docs/install.html
Tools: https://github.com/realworldocaml/book/wiki/Installation-Instructions
This link gives instructions for installing things like Tuareg (a useful emacs mode) and Merlin (advanced IDE features).

Installing Tuareg is pretty simple and will make your OCaml coding experience a lot nicer (though it's of course not necessary). Merlin is probably overkill unless you know what you're doing.

Learning:
Standard library documentation: http://caml.inria.fr/pub/docs/manual-ocaml/libref/index.html
The following documentation may be particularly useful as you work on your assignments.
Sets: http://caml.inria.fr/pub/docs/manual-ocaml/libref/Set.Make.html
Maps: http://caml.inria.fr/pub/docs/manual-ocaml/libref/Map.Make.html
Everything you need to know and more: http://caml.inria.fr/pub/docs/manual-ocaml/index.html
Code examples: http://ocaml.org/learn/tutorials/99problems.html
See also the CS51 Resources web page for OCaml books, references, and tutorials.

Coq resources
Download Coq.
Coq includes an IDE, CoqIDE. Alternatively, with Emacs, you can use Proof General.
Coq documentation.
Dafny resources
Dafny on Github.
Dafny on rise4fun.
Dafny in Visual Studio Code.
Haskell resources
Download The Haskell Platform.
List of Haskell tutorials. If you want to get more meta, see How to Learn Haskell.
haskell.org contains lots of reference information, language specification, etc.



Gödel's incompleteness theorems are two theorems of mathematical logic that demonstrate the inherent limitations of every formal axiomatic system capable of modelling basic arithmetic. These results, published by Kurt Gödel in 1931, are important both in mathematical logic and in the philosophy of mathematics. The theorems are widely, but not universally, interpreted as showing that Hilbert's program to find a complete and consistent set of axioms for all mathematics is impossible.



<select name="textbook">
SELECT textbook </option>

Aho, Sethi, Ullman - Compilers</option>
Brookshear - Computer Science - An Overview </option>
Brookshear - Theory of Computation </option>
Cohen - Intro to Computer Theory </option>
Davis - Computability, Complexity, and Languages </option>
Du and Ko - Problem Solving in Automata, Languages and Complexity </option>
Floyd and Beigel - The Language of Machines</option>
Gersting - Mathematical Structures for Computer Science</option>
Goddard - Introducing the Theory of Computation </option>
Homer - Computability and Complexity Theory </option>
Johnsonbaugh - Discrete Mathematics </option>
Kelley - Automata and Formal Languages </option>
Kinber and Smith - Theory of Computing</option>
Kozen - Automata and Computability </option>
Lewis and Papadimitriou - Elements of the Theory of Computation </option>
Linz - Formal Languages and Automata</option>
Martin - Introduction to Languages and the Theory of Computation</option>
Motwani, Ullman and Hopcroft - Intro to Automata Theory, Languages
and Computation </option>
Sipser - Introduction to the Theory of Computation
Sudkamp - Languages and Machines
Taylor - Models of Computation and Formal Languages
OTHER 



可计算的route
david hilbert 提出   数学系统公理化完整性
Kurt Gödel 证明无法完整性。 
alonzo church 和 alan turing 也证明形式系统不完整。 


数学的不确定性

构造一个形式系统， 让形式系统出现自证的悖论
同构的威力

galois theory


分点突破


galois theory的一点点思考：
根的对称性
群的对称性
分裂域 与  galois group 的对称性

大于5次  一点关系没有办法满足。 




lagrange root permutation
the roots of early group theory in the works of lagrange. 



The symmetric polynomials
Albert Girard (1590-1633), a flemish mathematician, published in 1629, in Amsterdam, a book called Invention nouvelle en l'algèbre, in which clear relations between roots and coefficients of polynomials were stated for the first time.



Edward Waring (1734-1793), an English mathematician, searched for the correlations between the degrees of the resolvent polynomials of a polynomial


The work of Lagrange
Joseph Louis Lagrange (1736-1813), a French mathematician, proved that a polynomial equation can be solved by radicals if a particularly related equation called the Lagrange resultant has degree less than the degree of the original equation. The proof involved a concept that later was to become fundamental to the theory of polynomial equations: that of the permutation of the roots. The Lagrange resultant is a polynomial constructed by means of a rational function of all possible permutations of the roots. An important characteristic of the theorem is that it is an existence theorem, as opposed to the previous methods that had all been concerned with the construction of a solving formula. Using this result, Ruffini produced in 1799 an erroneous proof that a polynomial equation od degree greater than 4 was not soluble by radicals. Abel independently found in 1824 a correct proof of the same theorem; thus the question of solving polynomial equation was in part settled. Still there are polynomials of degree greater than 4 that are clearly soluble, like tex2html_wrap_inline304. What remained to be done was to find a method to determine exactly which polynomial is soluble.


The work of Vandermonde and Gauss
Vandermonde proposed in 1770 that the key to solving a general polynomial equation
displaymath306
was represented by the roots of the equation tex2html_wrap_inline308. Gauss has the undoubted credit for having laid the first stone in the path that Galois successively followed. He showed that the roots of the polynomial equation tex2html_wrap_inline310 where p is prime are rational functions of the roots of a sequence of equations tex2html_wrap_inline314 where the coefficients of tex2html_wrap_inline316 are rational expressions of the roots of tex2html_wrap_inline318 and the degrees of the polynomials in the sequence are the all the prime numbers in the factorization of p-1.


It is not surprising that Galois was not understood even by the greatest mathematicians of the time. His papers make such a large number of unproved (though true) assumptions, that at first they really are incomprehensible. Modern Galois theory can be viewed from two possible angles, which are however closely linked: group theory and field theory. Following the group-theoretic approach, to each polynomial there corresponds a group of permutations of the roots; if the group contains a series of certain particular nested subgroups such that the intersection of the series is the identity element, this means that the identity is the only permutation of the roots that leave all the known rational relations between the roots valid, and hence it is possible to determine them. If the intersection contains other permutations apart from the identity, it means that all the known relations cannot determine the roots because even if we permute them with the permutations in the intersection the relations still hold. Taking the field-theoretic approach, for any polynomial having roots tex2html_wrap_inline322 and coefficients in the field F there exists a field F' such that the polynomial is reducible into linear factors in F'. Obviously any field tex2html_wrap_inline330 is such that the polynomial splits into linear factors. Any such field G is called a splitting field for f(x). It is clear that for any polynomial f(x) over F with roots tex2html_wrap_inline322 the field tex2html_wrap_inline342 is a splitting field for f. Very simply put, if F' is expressible by adjoining a finite number of radicals of the form tex2html_wrap_inline348 to the base field F, then it is possible to express the roots tex2html_wrap_inline322 in terms of the radical operations tex2html_wrap_inline354 and hence there is a radical formula to find the roots in terms of the coefficients. Thus Galois found necessary and sufficient conditions for the roots to be expressed as rational functions of the coefficients, settling the problem definitively.





-----some book---
Inside the Machine: An Illustrated Introduction to Microprocessors and Computer Architecture
Structure and Interpretation of Computer Programs
Design Patterns: Elements of Reusable Object-Oriented Software
Code: The Hidden Language of Computer Hardware and Software
Cracking the Coding Interview: 189 Programming Questions and Solutions
Code Complete: A Practical Handbook of Software Construction
Programming Pearls
The Pragmatic Programmer: Your Journey to Mastery
Code Simplicity: The Fundamentals of Software
Algorithms to Live By: The Computer Science of Human Decisions
Clean Code: A Handbook of Agile Software Craftsmanship
Think Like a Programmer: An Introduction to Creative Problem Solving
Introduction to Algorithms
Peopleware: Productive Projects and Teams
Soft Skills: The software developer’s life manual&nbsp;
Rapid Development: Taming Wild Software Schedules
Coders at Work: Reflections on the Craft of Programming
Don’t Make Me Think, Revisited: A Common Sense Approach to Web Usability
The Passionate Programmer: Creating a Remarkable Career in Software Development
Working Effectively with Legacy Code
The Self-Taught Programmer: The Definitive Guide to Programming Professionally
The Mythical Man-Month: Essays on Software Engineering
Refactoring: Improving the Design of Existing Code
HTML and CSS: Design and Build Websites
Learning Web Design: A Beginner’s Guide to HTML, CSS, JavaScript, and Web Graphics
Eloquent JavaScript: A Modern Introduction to Programming
You Don’t Know JS Yet: Get Started
Effective Java
Head First Java
Java Concurrency in Practice
Modern PHP
Head First PHP &amp; MySQL: A Brain-Friendly Guide
Eloquent Ruby
The Well-Grounded Rubyist
Python Crash Course: A Hands-On, Project-Based Introduction to Programming
Head First Python: A Brain-Friendly Guide
Learn Python 3 the Hard Way: A Very Simple Introduction to the Terrifyingly Beautiful World of Computers and Code
C# in Depth
C# 7.0 in a Nutshell: The Definitive Reference
C Programming Language
Practical C Programming: Why Does 2+2 = 5986?
Objective-C Programming: The Big Nerd Ranch Guide
Effective Objective-C 2.0: 52 Specific Ways to Improve Your IOS and OS X Programs
C++ Primer
Programming: Principles and Practice Using C++
Learning R: A Step-by-Step Function Guide to Data Analysis
R Cookbook: Proven Recipes for Data Analysis, Statistics, and Graphics
The Art of R Programming: A Tour of Statistical Software Design
SQL in 10 Minutes, Sams Teach Yourself
SQL Queries for Mere Mortals: A Hands-On Guide to Data Manipulation in SQL
Android Application Development All-in-One For Dummies
Head First Android Development: A Brain-Friendly Guide
iOS Programming: The Big Nerd Ranch Guide
Swift Programming: The Big Nerd Ranch Guide


--------


被识别叫作语言


A language is called a regular language if some finite automaton recognizes it.


regular operations union, concatenation, and star


nondeterministic finite automaton

EQUIVALENCE OF NFAS AND DFAS
1. 因为NFA会出现一个输入产生两个， 三个， 或多个状态的现象。 
2. 对状态做对自身的笛卡尔积，直到包含了所有状态，  然后重新绘制。 



 language is regular if and only if some nondeterministic finite automaton recognizes it


theorem 1:
 the class of regular languages is closed under the union operation.
构造一个NFA， 空指向两个子NFA。 证毕。

theorem 2:
The class of regular languages is closed under the concatenation operation.
同上， 构造NFA。 一个NFA 空连接另一个  NFA。


the class of regular languages is closed under the star operation.

在accept 状态用空连接 start 状态。 


A language is regular if and only if some regular expression describes it.


NONREGULAR LANGUAGES
找到违反他特征的例子。 

正则的特性
THE PUMPING LEMMA FOR REGULAR LANGUAGES
大于等于三的 string 中间存在可重复。 


---------

 context-free grammars::: recursive structure


pushdown automata, a class of machines recognizing the context-free languages

parse tree.


Any language that can be generated by some context-free grammar is called a context-free language(CFL).



fmcoirst, many CFLs are the union of simpler CFLs. If you must construct a CFG for
a CFL that you can break into simpler pieces, do so and then construct individual
grammars for each piece. These individual grammars can be easily merged into
a grammar for the original language by combining their rules and then adding
the new rule S → S1 | S2 | ··· | Sk, where the variables Si are the start variables
for the individual grammars. Solving several simpler problems is often easier
than solving one complicated problem.


computational model called pushdown automata.

finite automata 
为什么加stack, 可能就是为了方便识别递归。 
 context-free grammars and pushdown automata are
equivalent in power. Both are capable of describing the class of context-free languages. We show how to convert any context-free grammar into a pushdown automaton that recognizes the same language and vice versa. 


cfg to pda
stack 实现识别递归
: 递归的特征

(
 Theorem: If G is a CFG for a language L, 
then there exists a PDA for L as well.
● Idea: Build a PDA that simulates 
expanding out the CFG from the start 
symbol to some particular string.
● Stack holds the part of the string we 
haven't matched yet.)

THE PUMPING LEMMA FOR CONTEXT-FREE LANGUAGES
分成5部分。 
. It states that every context-free language has a special value called the pumping length such that all longer strings in the language can be “pumped.” This time the meaning of pumped is a bit more complex. It means that the string can be divided into five parts so that the second and the fourth parts may be repeated together any number of times and the resulting string still remains in the language.

找一个不是递归关系的。 

a(↑n)b(↑n)c(↑n)   他就不在



The languages that are recognizable by deterministic pushdown automata (DPDAs) are called deterministic context-free languages (DCFLs). 
This subclass of the context-free languages is relevant to practical applications, such as the design of parsers in compilers for programming languages, because the parsing problem is generally easier for DCFLs than for CFLs. This section gives a short overview of this important and beautiful subject.


 In a PDA, if there are multiple 
nondeterministic choices, you cannot
treat the machine as being in multiple 
states at once.
● Each state might have its own stack 
associated with it.
● Instead, there are multiple parallel 
copies of the machine running at once, 
each of which has its own stack



图灵机 与 lambda 演算的对等关系。 
图灵机 类似模拟 递归。 
turing machine lambda calculus equivalence

机算机核心问题
David Hilbert’s Entscheidungsproblem


The class of DCFLs is closed under complementation.
PROOF IDEA Swapping the accept and non-accept states of a DFA yields a new DFA that recognizes the complementary language, thereby proving that the class of regular languages is closed under complementation. 


加endmarked string也可被识别。 
A is a DCFL if and only if A⊣ is a DCFL.

DCFG
A deterministic context-free grammar is a context-free grammar such that every valid string has a forced handle.


单带 多带 TM。 
单带增加空间复杂度。 

o construct DFA DK, we’ll construct an equivalent NFA K and convert K to
DK1 via the subset construction introduced in Theorem 1.39.


对等关系  映射怎么找。 同构
every DCFG has an equivalent DPDA.

In the 1960s, theoretical research in computer science on regular expressions and finite automata led to the discovery that context-free grammars are equivalent to nondeterministic pushdown automata.[1][2][3] These grammars were thought to capture the syntax of computer programming languages. The first computer programming languages were under development at the time (see History of programming languages) and writing compilers was difficult. But using context-free grammars to help automate the parsing part of the compiler simplified the task. 



an LR(k) grammar is a context-free grammar such that the handle of every valid string is forced by lookahead k.



图灵机识别  011000#011000
Turing machine M1 computing on input 011000#011000

图灵机定义
 δ : Q × Γ−→Q × Γ × {L, R} is the transition function,

 Σ ⊆ Γ,


 表示法

  a Turing machine computes, changes occur in the current state, the current tape contents, and the current head location. A setting of these three items is called a configuration of the Turing machine. Configurations often are represented in a special way. For a state q and two strings u and v over the tape alphabet Γ, we write uqv for the configuration where the current state is q, the current tape contents is uv, and the current head location is the first symbol of v. The tape contains only blanks following the last symbol of v. For example,
1011q701111 represents the configuration when the tape is 101101111, the current state is q7, and the head is currently on the second 0. Figure 3.4 depicts a Turing machine with that configuration.


call a language Turing-recognizable if some Turing machine
recognizes it.1


call a language Turing-decidable or simply decidable if some
Turing machine decides it



every multitape Turing machine has an equivalent single-tape Turing machine



计算机发展史
John Mauchly
Oral history interview with J. Presper Eckert at Charles Babbage Institute, University of Minnesota, Minneapolis. Eckert, a co-inventor of the ENIAC, discusses its development at the University of Pennsylvania and the interaction of the personnel at the Moore School.
John W. Mauchly and the Development of the ENIAC Computer - by Asaf Goldschmidt and Atsushi Akera, An Exhibition in the Department of Special Collections Van Pelt Library, University of Pennsylvania
Mauchly: The Computer and the Skateboard. The only work to contain archival footage of John Mauchly speaking about the development of the ENIAC.
O'Connor, John J.; Robertson, Edmund F., "John Mauchly", MacTutor History of Mathematics archive, University of St Andrews.


, we prove one of the most philosophically important theorems of the theory of computation: There is a specific problem that is algorithmically unsolvable. Computers appear to be so powerful that you may believe that all problems will eventually yield to them. 


方法树， 在宽度上一个接收就接收。 都是拒绝的时候拒绝。 

TM：
识别器， 判定器， 转换器， 产生器， 枚举器。 


可识别等价可枚举。 
A language is Turing-recognizable if and only if some enumerator enumerates it. 


形式系统要么是不完备(complete)  要么不相容的（closure） 
不完备，


举一个复杂点的例子，也就是这一节最开头的那个表达式：

(5 - 3) * (4 + (2 * 3 - 5) * 6)
它可以被转化为一串语句：

{
    a = 2 * 3
    b = a - 5
    c = b * 6
    d = 4 + c
    e = 5 - 3
    e * d
}

王垠思想：
总有人提出一套套的所谓“方法论”或者“原则”，比如 Extreme Programming，Design Patterns，Agile，Pair Programming，Test Driven Development（TDD），DRY principle…… 他们把这些所谓方法论兜售给各个软件公司，鼓吹它们的各种好处，说使用这些方法，就可以用一些平庸的“软件工程师”，制造出高质量低成本的软件。这就跟减肥药的广告一样：不用运动，不用节食，一个星期瘦 20 斤。

你开头还不以为然，觉得这些肤浅的说法能造成什么影响。结果久而久之，这些所谓“方法论”和“原则”成为了整个行业的教条，造成了文化大革命一样的风气。违反这些教条的人，必然被当成菜鸟一样鄙视，当成小学生一样教育，当成反革命一样批斗。就算你技术比这些教条的提出者高不知道多少倍，也无济于事，因为他们已经靠着一张嘴占据了自己的地位。



可世界上就是有这样划算的行当，虽然写不出好的代码，对计算的理解非常肤浅，却可以通过嘴里说说，得到评价别人“代码质量”的权力，占据软件公司的管理层位置。久而久之，别人还以为他们是什么泰斗。你仔细看过提出 Design Pattern 的“四人帮”（GoF），做出过什么有实质价值的东西吗？提出“DRY Principle”的作者，做出过什么吗？再看看 Agile，Pair Programming，TDD 的提出者？他们其实不懂很多编程，写出文章和书来也是极其肤浅。

所谓“软件工程”，并不像土木工程，机械工程，电机工程，是建立在实际的，科学的基础上的。跟这些“硬工程”不一样，软件弄得不好不会出人命，也不会像芯片公司那样，出一个 bug 立即导致几十上百亿的损失。




想起这些借口我就想起一个笑话：两夫妻发现床上有跳蚤，身上被咬了好多包。去买了号称“杀伤率 100 %”的跳蚤药，撒了好多在床上。第二天早上起来，发现仍然被咬了好多新的包。妻子责怪丈夫，说他没看说明书就乱撒。结果丈夫打开说明书一看，内容如下：

本跳蚤药使用方法：

抓住跳蚤
掰开跳蚤的嘴
把药塞进跳蚤嘴里
合上跳蚤的嘴
我发现很多软件工程的所谓方法论失败之后的借口，跟这跳蚤药的说明书很像。





课程大纲
根据第二期课程的经验，我想对课程的内容做一个比以前详细的说明。之前一直对课程内容没有很多说明，一方面为的是留下自由发挥的空间，一方面是为了让学生有一定的神秘感，引发好奇心。但这么简单的说明似乎会让不知情的人误以为“已经学过这些东西”，有时候会发现一些人看了说明之后，自以为我教的内容他都会了。我只为他们感到可惜。

下面简要说一下课程的内容：

教学语言。课程目前使用 JavaScript 作为教学语言，但并不是教 JavaScript 语言本身，不会使用 JavaScript 特有的任何功能。课程教的思想不依赖于 JavaScript 的任何特性，它可以应用于任何语言，课程可以在任何时候换成任何语言。学生从零开始，学会的是计算机科学最核心的思想，从无到有创造出各种重要的概念，直到最后实现出自己的编程语言和类型系统。

课程强度。课程的设计是一个逐渐加大难度，比较辛苦，却很安全的山路，它通往很高的山峰。要参加课程，请做好付出努力的准备。在两个月的时间里，你每天需要至少一个小时来做练习，有的练习需要好几个小时才能做对。跟其他的计算机教学不同，学生不会因为缺少基础而放弃，不会误入歧途，也不会掉进陷阱出不来。学生需要付出很多的时间和努力，但没有努力是白费的。

曾经有一两个学生因为低估了学习的强度，同时又有其他重要任务，结果发现忙不过来，所以请合理的安排，不要在有其他重要任务的同时参加学习。

第一课：函数。跟一般课程不同，我不从所谓“Hello World”程序开始，也不会叫学生做一些好像有趣而其实无聊的小游戏。一开头我就讲最核心的内容：函数。关于函数只有很少几个知识点，但它们却是一切的核心。只知道很少的知识点的时候，对它们进行反复的练习，让头脑能够自如地对它们进行思考和变换，这是教学的要点。我为每个知识点设计了恰当的练习。

第一课的练习每个都很小，只需要一两行代码，却蕴含了深刻的原理。练习逐渐加大难度，直至超过博士课程的水平。我把术语都改头换面，要求学生不上网搜索相关内容，为的是他们的思维不受任何已有信息的干扰，独立做出这些练习。练习自成系统，一环扣一环。后面的练习需要从前面的练习获得的灵感，却不需要其它基础。有趣的是，经过正确的引导，好些学生把最难的练习都做出来了，完全零基础的学生也能做出绝大部分，这是我在世界名校的学生里都没有看到过的。具体的内容因为不剧透的原因，我就不继续说了。

第二课：递归。递归可以说是计算机科学（或数学）最重要的概念。我从最简单的递归函数开始，引导理解递归的本质，掌握对递归进行系统化思考的思路。递归是一个很多人自以为理解了的概念，而其实很多人都被错误的教学方式误导了。很多人提到递归，只能想起“汉诺塔”或者“八皇后”问题，却不能拿来解决实际问题。很多编程书籍片面强调递归的“缺点”，教学生如何“消除递归”，却看不到问题的真正所在——某些语言（比如 C 语言）早期的函数调用实现是错误而效率低下的，以至于学生被教导要避免递归。由于对于递归从来没有掌握清晰的思路，在将来的工作中一旦遇到复杂点的递归函数就觉得深不可测。

第三课：链表。从零开始，学生不依赖于任何语言的特性，实现最基本的数据结构。第一个数据结构就是链表，学生会在练习中实现许多操作链表的函数。这些函数经过了精心挑选安排，很多是函数式编程语言的基本函数，但通过独立把它们写出来，学生掌握的是递归的系统化思路。这使得他们能自如地对这类数据结构进行思考，解决新的递归问题。

与一般的数据结构课程不同，这个课程实现的大部分都是「函数式数据结构」，它们具有一些特别的，有用的性质。因为它们逻辑结构清晰，比起普通数据结构书籍会更容易理解。与 Haskell 社区的教学方式不同，我不会宗教式的强调纯函数的优点，而是客观地让学生领会到其中的优点，并且发现它们的弱点。学会了这些结构，在将来也容易推广到非函数式的结构，把两种看似不同的风格有机地结合在一起。

第四课：树结构。从链表逐渐推广出更复杂的数据结构——树。在后来的内容中，会常常用到这种结构。树可能是计算机科学中最常用，最重要的数据结构了，所以理解树的各种操作是很重要的。我们的树也都是纯函数式的。

第五课：计算器。在熟悉了树的基本操作之后，实现一个比较高级的计算器，它可以计算任意嵌套的算术表达式。算术表达式是一种“语法树”，从这个练习学生会理解“表达式是一棵树”这样的原理。

第六课：查找结构。理解如何实现 key-value 查找结构，并且亲手实现两种重要的查找数据结构。我们的查找结构也都是函数式数据结构。这些结构会在后来的解释器里派上大的用场，对它们的理解会巩固加深。

第七课：解释器。利用之前打好的基础，亲手实现计算机科学中最重要，也是通常认为最难理解的概念——解释器。解释器是理解各种计算机科学概念的关键，比如编程语言，操作系统，数据库，网络协议，Web 框架。计算机最核心的部件 CPU 其实就是一个解释器，所以解释器的认识能帮助你理解「计算机体系构架」，也就是计算机的“硬件”。你会发现这种硬件其实和软件差别不是很大。你可以认为解释器就是「计算」本身，所以它非常值得研究。对解释器的深入理解，也能帮助理解很多其它学科，比如自然语言，逻辑学。

第八课：类型系统。在解释器的基础上，学生会理解并实现一个相当高级的类型系统（type system）和类型检查器（typechecker）。这相当于实现一个类似 Java 的静态类型语言，但比 Java 在某些方面还要高级和灵活。我们的类型系统包含了对于类型最关键的要素，而不只是照本宣科地讲解某一种类型系统。当你对现有的语言里的类型系统不满意的时候，这些思路可以帮助你设计出自己的类型系统。学生会用动手的方式去理解静态类型系统的原理，其中的规则，却不含有任何公式。

类型系统的规则和实现，一般只会在博士级别的研究中才会出现，可以写成一本厚书（比如 TAPL 那样的），其中有各种神秘的逻辑公式。而我的学生从零开始，一节课就可以掌握这门技术的关键部分，实现出正确的类型系统，并且推导出正确的公式。有些类型规则是如此的微妙，以至于微软这么大的公司在 21 世纪做一个新的语言（TypeScript)，仍然会在初期犯下类型专家们早已熟知的基本错误。上过这个课程的很多同学，可以说对这些基础原理的理解已经超过了 TypeScript 的设计者，但由于接受的方式如此自然，他们有一些人还没有意识到自己的强大。

关于面向对象。虽然课程不会专门讲“面向对象”的思想，但面向对象思想的本质（去掉糟粕）会从一开头就融入到练习里。上过课的同学到后来发现，虽然我从来没直接教过面向对象，而其实他们已经理解了面向对象的本质是什么。在将来的实践中，他们可以用这个思路去看破面向对象思想的本质，并且合理地应用它。

奖励练习。途中我会通过“奖励练习”的方式补充其它内容。比如第二期的课程途中，我临时设计了一个 parser 的练习，做完了其它练习的同学通过这个练习，理解了 parser 的原理，写出了一个简单但逻辑严密的 parser。奖励练习之所以叫“奖励”，因为并不是所有学生都能得到这个练习，只有那些付出了努力，在其他练习中做到融会贯通，学有余力的学生才会给这个练习。这样会鼓励学生更加努力地学习。

如果理解了以上内容蕴含了什么，你可能就不会再问“这些我都学过了，我可不可以参加高级班”了，因为极少有人真的理解了以上内容。就算世界上最高级职位的一些程序员，大学里的教授，对于这些也有很多含糊不清的地方。我自己也通过讲授这些内容得到了启发。

一个朋友看了我的课程内容说，这不叫“基础班”，只能叫“大师班”。他不相信零基础的学生能跟上，但事实却是可行的。为什么不能即是“基础班”又是“大师班”呢？有句话说得好，大师只不过是把基础的东西理解得很透彻的人而已。我希望这个基础班能帮助人们获得本质的原理，帮助他们看透很多其它内容。所以上了“基础班”，可能在很长时间之内都不需要“高级班”了，因为他们已经获得了很强的自学能力，能够自己去探索未知的世界，攀登更高的山峰。



洪水算法
从源头一步一步向相邻点蔓延


-------------------------------------

通用图灵机可识别， 就是做同构。 
北京大学_刘田

可判定即可停机， 不会出现死循环是可计算

算法主要是学习那些东西是不可被计算的。 
可计算理论都是研究的不可计算的问题。 



正则（fa）的可计算问题:
	图灵机 可判定  正则。 
	nfa自动机是可计算的。 是可判定的。 在图灵模型上。 
	E(dfa)不接受任何串的dfa是可判定的。 
	把他转化成图的问题。 


	判定两个有穷自动机是否相等。 是可判定的。
	正则语言对称差运算是封闭的。 

上下文无关语言的可计算问题。 
	cfg的派生问题。是否可派生一个字串w， 证明方法， 变成Chomsky 范式， 因为n长度的Chomsky 范式是2n-1步，遍历所有2n-1的派生。 
	cfg 的空性问题。检查初始变原是否能产生一个终结符。 反向推导， 看终结符是否可以到原始变原。 
	给定两个cfg判定是否产生同一个语言。 （不可判定）
	每个cfl是可判定的。 


可判定即可停机， 不会出现死循环是可计算

图灵机的可计算问题
	图灵机模拟形式系统， 对等 lambda calculus. 
	检查一个图灵机是否接受一个串，（是不可判定的），反证法。 构造悖论。 如果证明机返回接受我拒绝， 如果证明机返回拒绝我接受。 
	看证明机， 能不能判定我是否接受一个w。 证明没有这样的证明机。 


可识别只能证明是这个东西， 不能证明不是这个东西。 可判定是可识别的特例。 

A和A的初都是可识别的， 那么他们是可判定的。 
图灵机的补不是图灵机可识别的。 存在非图灵可识别语言。 

语言类之间的关系

正则语言  上下文无关  图灵可判定语言  图灵可识别语言（递归可枚举）
  非图灵语言

	是不是可停机是不可判定。 

	图灵机的空性问题。 也是不可判定的。 构造一个图灵机只识别w。 则图灵机可判定w， 反证。 

检查一个图灵机是否有一个正则自动机？是不可判定的。 

线性界限自动机  是可判定的。 
E LBA是不可判定的。 

波斯特对应问题
归约  
可判定语言类在归约下是封闭的。 


rice定理。 
指标集是不可判定的。 


DTM决定性图灵机运行时间由输入长度来确定。 

log  幂函数的  反面。 
In mathematics, the logarithm is the inverse function to exponentiation. 
e (mathematical constant)

Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. Big O is a member of a family of notations invented by Paul Bachmann,[1] Edmund Landau,[2] and others, collectively called Bachmann–Landau notation or asymptotic notation.

In computer science, big O notation is used to classify algorithms according to how their run time or space requirements grow as the input size grows

https://en.wikipedia.org/wiki/Big_O_notation


输入长度与编码有关
输入规模  与  问题中的自然参数有关。 与编码无关。 



“P vs NP” 这个问题有它的理论价值，它是有趣的问题，里面的有些思路有启发意义，值得花些时间来了解。但计算机科学界长久以来都严重夸大它的重要性，把一个很普通的问题捧上了天，吹得神乎其神。

再加上图灵机模型在计算理论界的广泛使用，使得这门学问显得异常艰深。很多人看到图灵机就晕了，在课程上蒙混过关，考试完了就全忘了，根本无法理解里面的实质内容。正是因为很多人的不明觉厉，使得“P vs NP”登上了它在 CS 界的宝座。


“无穷”对于现实的问题是没有意义的。


微积分的重要性就是摒弃了无限这虚无飘渺的东西。 
计算复杂性概念

多带单带  单带空间上的限制带来复杂性。 
多带变单带：时间平方。 
非确定性与确定性图灵： 指数 。 


复杂性分类： 
P类 polynomial time
有向图可达性
	蛮力搜索。 
	宽度优先搜索。 
判断互素：
	蛮力法
	辗转相除法

识别是不是ｃｆｌ：
	蛮力法：
	动态规划：

NP类："nondeterministic polynomial time"
	哈密顿路径（送快递）：过每个点一次，且恰好一次。 
	每次问有没有， 有删边。 搜索归为判定问题。 
	验证机：

语言有多项式时间验证机， 就是NP类
	非确定图灵可判定语言。 
	团问题
	子集和问题（背包问题）

coNP类（补NP）  EXP类（指数类）


P成员资格可以快速判定的语言类
NP成员资格可以快速验证的语言类



空间复杂性 （space complex）
空间复杂性 与  时间复杂性。 

萨维奇定理
用确定性空间来模拟非确定型空间只有平方的增长。 
pspace类


亚线性空间比p还小（L类， NL 类）sub-linear
设多带  一条只读带， 一条可读写带， 一条输出带

NSPACE = CONSPACE


空间可构造

apt install clisp
clisp

  i i i i i i i       ooooo    o        ooooooo   ooooo   ooooo
  I I I I I I I      8     8   8           8     8     o  8    8
  I  \ `+' /  I      8         8           8     8        8    8
   \  `-+-'  /       8         8           8      ooooo   8oooo
    `-__|__-'        8         8           8           8  8
        |            8     o   8           8     o     8  8
  ------+------       ooooo    8oooooo  ooo8ooo   ooooo   8

Welcome to GNU CLISP 2.49.92 (2018-02-18) <http://clisp.org/>

Copyright (c) Bruno Haible, Michael Stoll 1992-1993
Copyright (c) Bruno Haible, Marcus Daniels 1994-1997
Copyright (c) Bruno Haible, Pierpaolo Bernardi, Sam Steingold 1998
Copyright (c) Bruno Haible, Sam Steingold 1999-2000
Copyright (c) Sam Steingold, Bruno Haible 2001-2018

Type :h and hit Enter for context help.

[1]> (+ 3 4)
7
[2]> (+ 3 4 8)
15




Computability theory asks questions such as: do there exist problems unsolvable by any
effective procedure — unsolvable by any program in any conceivable programming language on any computer?

 computability theory is concerned with the boundary between computability and uncomputability, and addresses questions such as:
• Can every precisely stated problem be solved by some effective procedure?
• What is the class of problems that can be solved by effective procedures and its
basic properties?
• What is the relationship between various problems that cannot be solved by effective procedu


There are many other notions of effective procedure than Turing machines, e.g.,
• Recursive functions as defined by Kleene [98]
• The lambda calculus approach to function definitions due to Church [22, 23].
• Random access machines [163]
• Markov algorithms [115]


the Church-Turing thesis is sometimes expressed in the following form:
1. All reasonable formalizations of the intuitive notion of effective computability are
equivalent;

. Turing machine computability is a reasonable formalization of effective computability.

effective procedure= algorithm


处处停机  可计算

On data representation 编码  能编回来能展现回来。 

 Given a set D, and a subset S ⊆ D. S is effectively decidable iff there
is an effective procedure which, when given an object x ∈ D, will eventually answer “yes”
if x ∈ S, and will eventually answer “no” if x 6∈ S.

 Given a set D, and a subset S ⊆ D. S is effectively enumerable iff
there is an effective procedure which, when given an object x ∈ D, will eventually answer
“yes” if x ∈ S, and will answer “no” or never terminate if x 6∈ S.


图灵的证明过程和哥德尔对不完备性定理的证明过程有很大的相似性，都需要对符号系统进行数字化的编码，同时，都要通过对角线方法来获得一个命题：我是不可判定的。

对角线方法
假设实数集可数。则实数能与自然数一一映射。
0 0.282728424871……1 
1.442282215825……2 
0.010101010101……3 
2.428245294258……4 
1.276343024642……5 1.234567891011…………可是我们可以创造一实数，整数部分与0不同，第一位与1的不同，第二位与2的不同……(如1.52744……等)，于是这个实数与以上列出来的都不相等，矛盾。所以实数集是不可数的。


Recall that computability theory is concerned with questions such as whether a problem is solvable at all, assuming one is given unlimited amounts of space and time. In
contrast, complexity theory is concerned with questions such as whether a problem can
be solved within certain limited computing resources, typically space or time.


to address such questions, one must have a precise definition of space and time costs.
Granted that, complexity theory asks questions such as:
• Which problems can be solved within a certain limit of time or space, and which
cannot?
• Are there resource limits within which a known combinatorial problem definitely
cannot be solved?
• Are there problems which inherently need more resources than others?
• What characteristics of problems cause the need for certain amounts of resources?
• What is the class of problems solvable within certain resource limits, and what are
the basic properties of this class?
• Given a problem, what is the complexity of its best algorithm?
• Do best algorithms always exist?
• Does adding more resources allow one to solve more problems



. There are a few problems whose exact complexity can be identified, but very few

Logspace ⊆ nlogspace ⊆ ptime ⊆ nptime ⊆ pspace = npspace ⊂ rec ⊂ re


At the conference Hilbert presented 23 unsolved mathematical problems. One of these,
the Entscheidungsproblem (decision problem), was described as follows:10
The Entscheidungsproblem is solved if one knows a procedure which will permit
one to decide, using a finite number of operations, on the validity, respectively the
satisfiability of a given logical expression.



In 1931 G¨odel showed his celebrated Incompleteness Theorem [54] stating, roughly,
that for any consistent, sufficiently strong formalization of number theory, there are
true propositions which cannot be proved in that formalization. To the experts this
result made it seem highly unlikely that the Entscheidungsproblem could have a positive
solution

 λ-expressions and Turing machines,

形式系统等价
 proved by Kleene, Turing, and others. In fact, one can
write compilers that turn a program in one formalism into a program in one of the
other formalisms that computes the same function, supporting what we have previously
called the Church-Turing thesis.

起了个复杂的名字

The first systematic investigation of time and space hierachies is due to Hartmanis,
Lewis, and Stearns [65, 64, 109] in the 1960’s, who coined the term “computational
complexity” for what we call complexity theory in this book

构造形式系统对等
proving the equivalence of WHILE with a variety of other computation models.



The WHILE Language
	Expressions 3 E, F ::= X (for X ∈ Vars)
		| d (for atom d)
		| cons E F   ###  This operation is called cons (short for “construct”.)
		| hd E       ### head 
		| tl E       ### tail
		| =? E F
	Commands 3 C, D ::= X := E
		| C; D
		| while E do C
	Programs 3 P ::= read X; C; write Y

reverse:

read X;
Y := nil;
while X do
	Y := cons (hd X) Y;
	X := tl X;
write Y


For reverse, if X is initially bound to input
	(d0.(d1.(···.(dn−1.(dn.nil))···)))
then Y is bound to
	(dn.(dn−1.(···.(d1.(d0.nil))···)))



The expression =? E F evaluates to true, if E and F evaluate to the same value, and to
false otherwise. Thus =? (nil.nil) (nil.nil) evaluates to true, and =? (nil.nil)
nil evaluates to false.




Definition 2.1.2 The function | •| : ID → IN defined by:
|d| =(1 if d ∈ A
|d1|+|d2| if d = (d1.d2)
denotes the size of a data value d ∈ ID


, e.g., (nil.nil) for a, (nil.(nil.nil)) for b, and ((nil.nil).nil) for c.


用while 模拟 if else
Z := E; (* if E then C *)
while Z do { Z := false; C };

Z := E; (* if E then C1 else C2 *)
W := true;
while Z do { Z := false; W := false; C1 };
while W do { W := false; C2 };



 Lists
As one can see from the example in subsection 2.1.3, elements of ID sometimes have
deeply nested parentheses that are hard to read; one has to resort to counting to parse
an element like ((a.(b.nil)).((d.(e.nil)).nil)).
Often the nesting has a certain regular structure, because we often express a list of
elements d0, d1,. . . , dn−1, dn as the tree (d0.(d1.(···.(dn−1.(dn.nil)) ···))). For
instance (a.(b.nil)) represents the list consisting of elements a, b. Therefore it would
be particularly convenient to have a short notation for this form. Hence the idea is to
use the notation (d0 ···dn) for the tree (d0.(d1.(···.(dn−1.(dn.nil)) ···))). Then the
tree (a.(b.nil)) can be written (a b) in short notation and, as another example, the
tree ((a.(b.nil)).((d.(e.nil)).nil) can be written ((a b) (d e)).
This is introduced in the following definition



 The function | •| : ID → IN defined by:
|d| =
(1 if d ∈ A
|d1|+|d2| if d = (d1
.d2)
denotes the size of a data value d ∈ ID. 


Define n = niln, where
nil0 = nil = ()
niln+1 = (nil.niln
) = (nil ... nil
| {z }
n+1 times
)
and let N = {n|n ∈ IN}. The elements of N are called numerals.

WHILE has only one atom, so how can we compute with numbers? One idea is to
represent the number n by a list of length n.




read X; (* succ *)              read X; (* pred *)
    Y := cons nil X;                Y:=tl X;
write Y                         write Y



read XY; (* add X Y *)
	X := hd XY;
	Y := tl XY;
	while X do
		Y := cons nil Y;
		X := tl X;
write Y

语法糖
Syntactic sugar: some useful macro notations

 cons* E1 ··· En



A compiler is a program transformer which takes a program and translates it into an
equivalent program, possibly in another language



An interpreter takes a program and its input data, and returns the result of applying the program to that input.
Language L is equivalent to language M, written L ≡ M, if language L and language M
can simulate each other. 



• : WHILE−programs → WHILE−data


Mapping WHILE programs to their data representations.

read Vi; C; write Vj = ((vari)C(varj))
C;D = (;CD)
while E do C = (whileEC)
Vi:=E = (:= (vari)E)
Vi = (vari)
d = (quoted)
cons E F = (consEF)
hd E = (hdE)
tl E = (tlE)
=? E F = (=? EF)


Suppose we are given three programming languages:
• A source language S,
• A target language T, and
• An implementation language L.


In order to be meaningful a diagram must be “well-formed,” that is satisfy some
natural constraints:
1. All languages appearing bottom-most in the diagram must be executable (either
because they are machine languages, or because implementations are known to
exist even though not shown in the diagram).
2. Let us define language L to match language M, written L v M, to mean that any
L-program is also an M-program, and has the same semantics. A special case: L v
L, that is, any language matches itself.
3. The second constraint is that any subdiagram of either of the following two forms:



Self-interpretation: Universal Programs for WHILE and I


A partial function f : ID → ID⊥ is WHILE computable iff there is a
WHILE program p such that f = [[p]], i.e. for all d,e ∈ ID:
1. If f (d) = ⊥ then [[p]](d) = ⊥.
2. If f (d) = e ∈ ID then [[p]](d) = e.


For a language L
if there is some Turing Machine that accepts every string in L
and rejects every string not in L, then L is a decidable
language
if there is some Turing machine that accepts every string in L
and either rejects or loops on every string not in L, then L is
Semi-decidable or computably enumerable



L = {0n1n: n ≥ 0} is not regular.
limitation of FA related to fact that they can only “remember” a bounded amount of information。



Automaton scans an input from left to right - at each step it may push a symbol onto the stack, or pop the stack. It cannot read other elements of the stack.

Automaton scans an input from left to right - at each step it may push a symbol onto the stack, or pop the stack. It cannot read other elements of the stack.



Every CFL is generated by a CFG in Chomsky Normal Form

Proof: Transform any CFG into equivalent CFG in CNF. 4 steps:
add a new start symbol, which can produce prior start
remove “-productions” A → 
eliminate “unit productions” A → B
convert remaining rules into proper form


abcdefg convert to a(b(c(d(e(fg)))))


学习各形式系统都有那些限制，新系统解决了那些限制 。 
limitation of NPDA related to fact that their memory is stack-based (last in, first out)


everything we can compute on a physical
computer can be computed on a Turing
machine.



The real numbers R are NOT countable (they are
“uncountable”).
How do you prove such a statement?
assume countable (so there exists function f
from N onto R)
derive contradiction (“construct” an element
not mapped to by f )
technique is called diagonalization (Cantor)


Theorem
HALT is not decidable (undecidable).
Proof will involve the following
Suppose there’s some TM H that decides HALT. Using this we will get a contradiction. You’ll need to believe that TMs can simulate other TMs, also can be composed with each other.



Language Decided
We say a TM decide a language L if it accepts all strings in L and rejects all strings not in L. (Here, the TM should not accept any string not in L, but when we say a TM accept a word w, it may accept any other word also)

Language Recognized
We say a TM recognizes language L if it accepts all strings in L. It may or may not reject any string not in L- i.e., it might go into an infinite loop in case of non-acceptance. (It will never accept a string not in L)


rice定理等价停机问题
In this note I will show a theorem which states that a very large family of
problems are all undecidable. Our result, known as Rice’s theorem after Henry
Rice who proved the result in 1953 [2], states that if S is a non-trivial property
of Turing-recognizable languages, then the problem
Given a TM M, does L(M) have the property S?
is undecidable.
What's a trivial property?

A "trivial" property is one that holds either for all languages or for none.
牛逼 

David Richerby
top 0.10% overall
Apparently, this user prefers to keep an air of mystery about them
https://cs.stackexchange.com/users/9550/david-richerby

好书
structure and interpretation of computer programs pdf



 These models of computation can’t solve every problem.


what does this do?
((lambda (x) (x x))
(lambda (x) (x x)))



(define (troll)
(if (halt? troll)
; if halts? says we halt, infinite-loop
((lambda (x) (x x)) (lambda (x) (x x)))
; if halts? says we don't, return a value
#f))
(halt? troll)



Run it with a copy of itself.
(define (fact inner-fact n)
(if (= n 0)
1
(* n
(inner-fact inner-fact (- n 1)))))



I have never done anything “useful.”
— Godfrey Harold Hardy (1877–1947),
A Mathematician’s Apology (1940)


What This Course Is All About
Computation: What is computation?
Computability: What can be computed?
• There are problems that cannot be computed.
• In fact, most problems cannot be computed.


Recommended reading
* Hopcroft, J.E., Motwani, R. & Ullman, J.D. (2001). Introduction to automata theory, languages, and computation. Addison-Wesley (2nd ed.).
* Hindley, J.R. & Seldin, J.P. (2008). Lambda-calculus and combinators, an introduction. Cambridge University Press (2nd ed.).
Cutland, N.J. (1980). Computability: an introduction to recursive function theory. Cambridge University Press.
Davis, M.D., Sigal, R. & Weyuker, E.J. (1994). Computability, complexity and languages. Academic Press (2nd ed.).
Sudkamp, T.A. (2005). Languages and machines. Addison-Wesley (3rd ed.).



What This Course Is All About (concluded)
Applications: Intractability results can be very useful.
• Cryptography, digital currency, and security.
• Approximations.
• Pseudorandom number generation and
derandomization.
• Conjectures about nature.



指数增长
1 1 9 362,880
2 2 10 3,628,800
3 6 11 39,916,800
4 24 12 479,001,600
5 120 13 6,227,020,800
6 720 14 87,178,291,200
7 5040 15 1,307,674,368,000
8 40320 16 20,922,789,888,000



What Is Computation?
• That can be coded in an algorithm.a
• An algorithm is a detailed step-by-step method for
solving a problem.
– The Euclidean algorithm for the greatest common
divisor is an algorithm.
– “Let s be the least upper bound of compact set A” is
not an algorithm.
– “Let s be a smallest element of a finite-sized array”
can be solved by an algorithm.
– How about passing the Turing test?



• A Turing machine (TM) is a quadruple M = (K, Σ, δ, s).
• K is a finite set of states.b
• s ∈ K is the initial state.
• Σ is a finite set of symbols (disjoint from K).
Σ includes  (blank) and ✄ (first symbol).c

δ : : K × Σ → (K ∪ {h, “yes”, “no”}) × Σ × {←, →, −} is a transition function.
–
← (left),
→ (right), and
− (stay) signify cursor movements.

Given current state q ∈ K and current symbol σ ∈ Σ,
δ(q, σ)=(p, ρ, D).
– It specifies:
∗ The next state p;
∗ The symbol ρ to be written over σ;
∗ The direction D the cursor will move afterwards


• For convenience, δ(q, ✄)=(·, ✄, →) for every q ∈ K.
– So the cursor never falls off the left end of the string.
• Think of the program as lines of codes:
δ(q1, σ1)=(p1, ρ1, D1),
δ(q2, σ2)=(p2, ρ2, D2),
.
.
.
δ(qn, σn)=(pn, ρn, Dn).


• The specification of a configuration is sufficient for the
computation to continue as if it had not been stopped.
– What does your PC save before it sleeps or
hibernates?
– Enough for it to resume the work later.
• Similar to the concept of state in Markov processes.


configuration: 用于保存现场。 
A configuration is a triple (q, w, u):
– q∈K.
– w∈Σ∗ is the string to the left of the cursor(inclusive).
–u∈Σ∗ is the string to the right of the cursor.
• Note that (w, u) describes both the string and the cursor
position.


“Physical” Interpretations
• The tape: computer memory and registers.
– Except that the tape can be lengthened on demand.
• δ: program.
– A program has a finite size.
• K: instruction numbers.
• s: “main()” in the C programming language.
• Σ: alphabet, much like the ASCII code.


Given current state q ∈ K and current symbol σ ∈ Σ,
δ(q, σ)=(p, ρ, D).
– It specifies:
∗ The next state p;
∗ The symbol ρ to be written over σ;
∗ The direction D the cursor will move afterwards


• Assume the state is q and the symbol under the cursor σ.
• The line of code that matches (q, σ) is executed.a
• Then the process is repeated.


The Concept of Configuration
•
A configuration a is a complete description of the
current state of the computation



• A string is a palindrome if it reads the same forwards and backwards (e.g., 001100).


Theorem 1 (Hennie, 1965) palindrome on single-string
TMs takes Ω^(2) steps in the worst case.

• This happens rarely and is model dependent.
– Searching, sorting, palindrome, matrix-vector
multiplication, etc.

Stephen Kleene (1909–1994)


The two words in the language I most respect
are Yes and No.
— Henry James (1843–1916),
The Portrait of a Lady (1881)


• Let
M be a TM such that for any string
x:
– If x∈L, then M(x) = “yes.”
– If x∈L, then M(x) = “no.” 
• We say M decides L.

• If there exists a TM that decides
L, then L is said to be recursive a or decidable.


• The set of palindromes over any alphabet is recursive.
a
– palindrome cannot be solved by finite state
automata.
– In fact, finite-state automata are equivalent to
read-only, right-moving TMs.



• The set of C programs that do not contain a while, a
for, or a goto is recursive.
a
• But, the set of C programs that do not contain an
infinite loop is not recursive (see p. 137).

a There is a program that will halt and it returns “yes” if and only if
the input C code does not contain any of the keywords. bSo there is no algorithm that will answer correctly in a finite amount
of time if a C program will run into an infinite loop on some inputs.

能识别但不能判定的：
• If L is accepted by some TM, then
L is said to be recursively enumerable or semidecidable.


Emil Post (1897–1954)

 The set of C program-input pairs that do not run into
an infinite loop is recursively enumerable.
– Just run its binary code in a simulator environment.
– Then the simulator will terminate if and only if the C
program will terminate.
– When the C program terminates, the simulator
simply exits with a “yes” state.
• The set of C programs that contain an infinite loop is
not recursively enumerable.
a。



Church’s Thesis or the Church-Turing Thesis
(concluded)
• Many other computation models have been proposed.
– Recursive function,
a
λ calculus,
b boolean circuits,
c
formal language,
d assembly language-like RAM,
e
neural networks,
f and extensions of the Turing
machine (more strings, two-dimensional strings, etc.).
• All have been proved to be equivalent.



Decision problems
Entscheidungsproblem means “decision problem”. Given
! a set S whose elements are finite data structures of
some kind
(e.g. formulas of first-order arithmetic)
! a property P of elements of S
(e.g. property of a formula that it has a proof)
the associated decision problem is:
find an algorithm which
terminates with result 0 or 1 when fed an element s ∈ S
and yields result 1 when fed s if and only if s has property P.


处处停机
Algorithms, informally
No precise definition of “algorithm” at the time Hilbert
posed the Entscheidungsproblem, just examples.
Common features of the examples:
! finite description of the procedure in terms of
elementary operations
! deterministic (next step uniquely determined if there
is one)
! procedure may not terminate on some input data,
but we can recognize when it does terminate and
what the result is.


Common features of the examples:
! finite description of the procedure in terms of
elementary operations
e.g. multiply two decimal digits by
looking up their product in a table
! deterministic (next step uniquely determined if there
is one)
! procedure may not terminate on some input data,
but we can recognize when it does terminate and
what the result is.

The Halting Problem
is the decision problem with
! set S consists of all pairs (A, D), where A is an algorithm and
D is a datum on which it is designed to operate;
! property P holds for (A, D) if algorithm A when applied to
datum D eventually produces a result (that is, eventually
halts—we write A(D)↓ to indicate this).

多带图灵机

Turing Machines with Multiple Strings

Time seemed to be the most obvious measure of complexity.
— Stephen Arthur Cook (1939–)



I keep bumping into that silly quotation
attributed to me that says
640K of memory is enough.


Michael O. Rabin
Dana Stewart Scott



Universal Turing Machinea：：：
U(M; x) = M(x).
U is like a modern computer, which executes any valid
machine code, or a Java virtual machine, which executes
any valid bytecode.




H Is Not Recursivea
• Suppose H is recursive.
• Then there is a TM MH that decides H.
• Consider the program D(M) that calls MH:
1: if MH(M; M) = “yes” then
2:  ; {Writing an infinite loop is easy.}
3: else
4: “yes”;
5: end if


Self-Loop Paradoxes

规约即转换
• Suppose we are asked to prove that
L is undecidable.
• Suppose L (such as H) is known to be undecidable.
• Find a computable transformation R (called reduction a) from L to
L such that 
b ∀ x { x ∈ L if and only if R ( x ) ∈ L }.
• Now we can answer “
x ∈ L?” for any
x by answering
“ R ( x ) ∈ L?” because it has the same answer.
• L is said to be reduced to
L.


If
L is recursive, then so is L¯. （L complement）



RE: The set of all recursively enumerable languages.
coRE: The set of all languages whose complements are recursively enumerable.
R: The set of all recursive languages.

Note that coRE is not RE(complement).

trivial property:::::
– Answer to a trivial property is always “yes”or always“no.”



Undecidability in Logic and Mathematics
• First-order logic is undecidable (answer to Hilbert’s
(1928) Entscheidungsproblem).
a
• Natural numbers with addition and multiplication is
undecidable.
b
• Rational numbers with addition and multiplication is
undecidable.
c



Elementary theory of groups is undecidable.


Julia Hall Bowman Robinson (1919–1985)

Alfred Tarski (1901–1983)
香农定律：
如果把网络带宽比喻为车道宽度，那么网速就好比汽车在车道上行驶的速度。汽车在车道上行驶得快或者不快，要受限于车道宽度的大小，车道上正有多少辆汽车在行驶等诸多干扰性因素。

图中标明了香农三定理的位置，第一定理和第三定理对应的是压缩理论，而第二定理也就是鼎鼎有名的香农容量，对应的是传输理论。香农第一定理——无失真信源编码定理香农第一定理在编码领域的地位，无异于倚天屠龙记中的九阳神功。它给出了在无损情况下，数据压缩的临界值。不要小瞧这简简单单的一句话，正因为给出了理论上的下界，所以才会产生各式各样的编码办法。在无损压缩的情况下，压缩任何东西所需要的比特数都大于香农第一定理所给出的值。可以说，任何压缩方法都是在这个圈子里兜兜转转，是跳不出来的。接下来，我详细说一下香农第一定理的数学化表述：

The Relation between Nondeterministic and Deterministic Space Is Only Quadratic

Reductions and Completeness


Degrees of Difficulty
• When is a problem more difficult than another?
• B reduces to A if:
– There is a transformation R which for every problem
instance x of B yields a problem instance R(x) of A.a
– The answer to “R(x) ∈ A?” is the same as the answer to “x ∈ B?”
– R is easy to compute.
• We say problem A is at least as hard asb problem B if B reduces to A.


 This makes intuitive sense: If A is able to solve your
problem B after only a little bit of work of
R, then A
must be at least as hard.
– If A is easy to solve, it combined with
R (which is also easy) would make B easy to solve, too.
a
– So if B is hard to solve, A must be hard (if not harder), too!


• Our definition says in this case B is a special case of A.
b
• Hence A is harder.

hamiltonian path

hamiltonian path



The required properties of a satisfactory formal system are that it be

complete -- it must be possible either to prove or to disprove any proposition that can be expressed in the system.
consistent -- it must not be possible to both prove and disprove a proposition in the system.



Kurt Gödel explored the very notions of completeness and consistency. He invented a numbering scheme (Gödel numbers) that allowed him to express proofs as numbers (much as we might consider a computer program to be a very large binary number). He was able to prove the following result:

If it is possible to prove, within a formal system, that the system is consistent, then the formal system is not, in fact, consistent.

Or, equivalently,

If a formal system is consistent, then it is impossible to prove (within the system) that it is consistent.




We prefer to give up completeness rather than consistency, because in a consistent system any proposition can be proven.


Gödel left open the possibility that we could somehow distinguish between the provable propositions and the unprovable ones. Ideally, we would like to have a mechanical (algorithmic) theorem-proving procedure. Alan Turing invented Turing machines in an attempt to solve this problem. With the Halting Problem, he showed that we cannot, in all cases, distinguish between soluable and insoluable problems.


Other mathematicians, working with very different models of computation, ended up with very similar results. One of these was Alonzo Church, who invented recursive function theory.


 recursive function theory



 The textbook describes these functions as being over the natural numbers I={0,1,2,3,...}. A better way to look at recursive functions, though, is as pure symbol systems. Numbers are not used in the system; rather, we use the system to construct both numbers and arithmetical functions on numbers. In other words, it's a different numbering system, in the same way that Roman numerals are different. The correspondence goes like this:

z(x)=0, s(z(x))=1, s(s(z(x)))=2, s(s(s(z(x))))=3, ...




The zero function: z(x)=z(y) for all x,yis a member ofI. (This is our "zero"; it is written as a function so we don't have to introduce constants into the system.)
The successor function: s(x). Informally, this means "x+1". Formally, it doesn't "return a value", it just sits there: the result of s(x) is s(x).
For convenience, we make the following "abbreviations":
0 is shorthand for z(x).
1 is shorthand for s(z(x)).
2 is shorthand for s(s(z(x))).
3 is shorthand for s(s(s(z(x))).
...and so on.
The projector functions:
p1(x) = x.
p1(x, y) = x.
p2(x, y) = y.




Composition and Recursion

The recursion must be guaranteed to terminate. To ensure this, the function must carry along an extra parameter that is "decremented" each time the function is called (s(x) is replaced by x), and halts the recursion when it reaches "zero" (z(x)). That is,
f(..., z(x)) = ...

f(..., s(x)) = ... f(..., x), ...

The recursive function must appear only once in the definiens (right hand side of the definition). This restriction prevents various forms of "fancy" recursion.




Primitive Recursion
The second building operation is called primitive recursion. To those not used to it, it can be far from intuitive; to those who have practiced with it, it is fundamental and elegant.

Function h is defined through functions f and g by primitive recursion when

h(x,0) = f(x)
h(x,s(y)) = g(x,h(x,y))



The key new feature here is the use of a previously defined function, add, in the definition of a new function. We skip the step of playing around with the pi functions to pick out the right parts, and go right to the simplified form.

multiply(x, s(z(x))) = x
multiply(x, s(y)) = add(x, multiply(x, y))



阿克曼函数
1920年代后期，数学家大卫·希尔伯特的学生Gabriel Sudan和威廉·阿克曼，当时正研究计算的基础。Sudan发明了一个递归却非原始递归的Sudan函数。1928年，阿克曼又独立想出了另一个递归却非原始递归的函数。[1]


I've always used the two terms interchangeably, but your textbook makes a distinction, so here it is.

Turing's thesis. Anything that is computable can be computed by a Turing machine. There does not and cannot exist a machine that can compute things a Turing machine cannot compute.

Church's thesis. All the models of computation yet developed, and all those that may be developed in the future, are equivalent in power. We will not ever find a more powerful model.


Complexity Theory
Complexity theory concerns itself with two kinds of measures: time and space.


Time complexity is a measure of how long a computation takes to execute. For a Turing machine, this could be measured as the number of moves required to perform a computation. For a digital computer, it could be measured as the number of machine cycles required for the computation.

Space complexity is a measure of how much storage is required for a computation. For a Turing machine, the obvious measure is the number of tape squared used; for a digital computer, the number of bytes used.


For any given input size, different inputs typically require different amounts of space and time. Hence we can discuss for either the average case or for the worst case. Usually we are interested in worst-case complexity because

It may difficult or impossible to define an "average" case. For many problems, the notion of "average case" doesn't even make sense.
 
It is usually much easier to compute worst-case complexity.
 


Polynomial-Time Algorithms
A polynomial-time algorithm is an algorithm whose execution time is either given by a polynomial on the size of the input, or can be bounded by such a polynomial. Problems that can be solved by a polynomial-time algorithm are called tractable problems.

For example, most algorithms on arrays can use the array size, n, as the input size. To find the largest element in an array requires a single pass through the array, so the algorithm for doing this is O(n), or linear time.

Sorting algorithms usually require either O(n log n) or O(n^2) time. Bubble sort takes linear time in the best case, but O(n2) time in the average and worst cases. Heapsort takes O(n log n) time in all cases. Quicksort takes O(n log n) time on average, but O(n2) time in the worst case.


Probably all the programming tasks you are familiar with have polynomial-time solutions. This is not because all practical problems have polynomial-time solutions. Rather, it is because your courses and your day-to-day work have avoided problems for which there is no known practical solution.



Integer Bin Packing
Suppose we are given a set of n positive integers. Our task is to arrange these integers into two piles, or "bins", so that the sum of the integers in one pile is equal to the sum of the integers in the other pile.

For example, given the integers

(19, 23, 32, 42, 50, 62, 77, 88, 89, 105, 114, 123, 176)

These numbers sum to 1000. Can they be divided into two bins, bin A and bin B, such that the sum of the integers in each bin is 500?




Boolean Satisfiability
Suppose you have n Boolean variables, named A, B, C, ..., and you have an expression in the propositional calculus (that is, you can use and, or, and not to form the expression.) Is there an assignment of truth values to the variables (e.g. A=true, B=true, C=false, ....) that will make the expression true?



Additional NP Problems
The following problems all have a polynomial-time solution on a nondeterministic machine, but an exponential-time solution on a deterministic machine. There are literally hundreds of additional examples.

The travelling salesman problem
A salesman, starting in Harrisburg, wants to visit every capital city in the 48 continental United States, returning to Harrisburg as his last stop. In what order should he visit the capital cities so as to minimize the total distance travelled?

The Hamiltonian circuit problem
Every capital city has direct air flights to at least some other capital cities. Our intrepid salesman wants to visit all 48 capitals, and return to his starting point, taking only direct air flights. Can he find a path that lets him do this?

Equivalence of regular expressions
Do two distinct regular expressions represent the same language?

Intersection of finite automata
Given a set of finite automata M1, M2, M3, ..., Mn, all over the same alphabet A, is there some string in A* that is accepted by all of these automata?

Linear programming
You have on hand X amount of butter, Y amount of flour, Z eggs, etc. You have cookie recipies that use varying amounts of these ingredients. Different kinds of cookies bring different prices. What mix of cookies should you make in order to maximize profits?

This type of problem is sufficiently important that entire college courses are devoted to it, usually in the College of Business.




图灵机对纸笔进行数学运算的过程。 打字机。 


https://wiki.python.org/moin/TimeComplexity

十进制变2进制  一直除


表达式转前缀表达


表达式转换  

In mathematics and computer science, a class of objects or methods exhibits recursive behavior when it can be defined by two properties:

A simple base case (or cases) — a terminating scenario that does not use recursion to produce an answer
A recursive step — a set of rules that reduces all successive cases toward the base case.
For example, the following is a recursive definition of a person's ancestor. One's ancestor is either:

One's parent (base case), or
One's parent's ancestor (recursive step).




递归和stack一个东西， 不同表现。 
调用栈。 


递归和树是否等价。 



虚拟货币运作之本质即是将每笔交易记载于区块中，形成一条链性结构，然而这样的「直线特性」也就成了最佳的溯源工具。因此与其说虚拟货币具有匿名性，不如说它只具有伪匿名性（pseudo-anonymity）。也就是虽然表面上具有匿名性，但实际上透过层层破解及科技溯源仍可找到实际使用者位置。


树
   画树干
   左转画树干reset position
   右转画树干reset pisition 
   reset pisition




sierpinski谢尔宾斯基三角形


汉诺塔

把塔  从三个柱子的最左边一个柱子， 移到最右边一个柱子上。 要求大盘不能在小盘上。 
是个递归，把下一个和上一个移出来， 组成一个组，成上一个， 一直进行下去，到次最大， 然后把次最大的组成一个组， 
1号  2号  3号
把顶上的移到2号， 移最大到3， 
把顶上移到1号， 移最大到3， 
把顶上移到2号 移最大到3号， 




兑最少货币。 
贪心策略


elbonina.
找零兑换问题



最短路径 是动态规划


分阶段， 每一阶段的值， 下一阶段到上一阶段的值，一直递归下去，找到最优。

每一阶段都有状态，  

三峡学院 关文忠 运筹学  很好《运筹学》、《统计学
动态规划 dynamic Programming 是求解多阶段决策过程multistep decision process最优化的一种数学方法。 它将问题的整体按时间或空间的特征分成若干个前后衔接的时空阶段， 把多阶段决策问题表示 为前后有关的一系列单阶段决策问题， 然后逐个求解， 从而求出整个问题的最优决策序列。 它强调了时间和空间的连续性。 
贝尔曼
Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.

管理运筹学 关文忠，韩宁鑫主编

1阶段  2状态变量  3决策变量  4 策略 5 状态转移方程 6 指标函数与最优值函数 7 边界条件


阶段变量
每阶段节点可称状态
每一状态和下一阶段 允许策略的集合  决策变量。



策略 （决策集）
最优化原理是动态规划的基本原理， 最优策略的子策略也是最优的。 


动态规划的基本思想
f(i)= min(d(i,j) + f(j))
f(e) = 0
动态规划的关键在于写出基本递推关系式， 和恰当的边界条件。 



在多阶段决策过程中， 即把当前阶段和未来阶段分开， 又把当前效益和未来阶段结合起来。 

从前往后推  和 从后往前推


s1={A}  s2 = {B1, B2} s3={C1, C2, C3, C4} s4={D1, D2}



动态规划 有逆序  有顺序解法。 


资源分配问题  工厂是阶段。 资源数量是状态。 状态之间存在的配对关系是策略。 
设备负何问题  季度是阶段。 完好数量是状态。 状态之间可存在的配对关系是策略。 
生产库存问题  
背包问题。 
非线性规划求解。 


最小支撑树  最大流问题  最短路问题   中国邮路问题
基本算法
最小支撑树  避圈法破圈法   
最大流问题  ford-fulkerson标号算法
最短路问题  两点间最短路dijkstra标号算法   各点间最短路矩阵算法
中国邮路问题  管梅谷重要结论


图点和点连线。 不带箭头为边edge， 带箭头叫弧arc。 
无向图， 有向图
有向图去掉箭头为有向图的基础图
G={V, E}, G={V, A}
e(edge) = [u,v], 一条两点决定。 关联边。 
点相邻。 两个端点重合称为环。 
两点多个连接， 多重边。 
无环， 无多重边的图， 简单图。 
无环， 有多重边的图， 多重图。 

次， 奇点， 偶点， 孤立点， 悬挂点， 悬挂边。 

边续统假设不成立
Mathematicians Measure Infinities, Find They're Equal

The problem was first identified over a century ago. At the time, mathematicians knew that “the real numbers are bigger than the natural numbers, but not how much bigger. Is it the next biggest size, or is there a size in between?” said Maryanthe Malliaris of the University of Chicago, co-author of the new work along with Saharon Shelah of the Hebrew University of Jerusalem and Rutgers University.


In their new work, Malliaris and Shelah resolve a related 70-year-old question about whether one infinity (call it p) is smaller than another infinity (call it t). They proved the two are in fact equal, much to the surprise of mathematicians.



悬挂点   
图论来源线性规划    
如何分配资源
如何有效利用资源 
图论是运筹学的一部分。 

次为1的点叫做悬挂点
与悬挂点相关的边叫作悬挂边

次数为0的点叫做孤立点。 


所有顶点的次数是边数的2倍。 
因为次数之和是偶数， 所以， 奇点的个数为偶数。 


边各不相同为链。 
封闭的链称为圈。


点和边的交错序列中， 若边各不相同称为链
封闭的链称为圈
在链中如果点也各不相同称为路。 
点点都有一条链连通  连通图


完全图  子图  支撑图（部分图）
任意两点间都有边相连， 完全图。 
子图， 点是子集， 边是子集 
支撑图（部分图）  点集相同， 边集是子集。 


化学品放库房问题： 染色问题  
找出次数最大的点， 把它与不相邻的点组成新的点集， 再重复上边动作。 


10名研究生  6门课  不能一天考。


无圈的连通图叫做树。树图 


任何树图必有悬挂点
任何n个点的树图恰好n-1条边
什么样的何具有n个点n-1条边的连通图是树
树图任意两点， 只有一条链
树图任意两点添加一条边正好构成一个圈
T是G的支撑图，又是树图，则称支撑树。 部分树。 


赋权图  一个支撑树的所有边的权之各叫做支撑树的权。 
最小支撑树， 是一个图是权最小的支撑树。 

spanning tree

图G有支撑树的充分必要条件为图G是连通的

两个点权最小的边， 一定包含中最小支撑树中。 
把图的所有点分成两个集合， 则两个集合的最小权的边一点包含在最小支撑树中。 


求解最小支撑树的法则。 

避圈法
破圈法

避圈法
根据
（两个点权最小的边， 一定包含中最小支撑树中。 
把图的所有点分成两个集合， 则两个集合的最小权的边一点包含在最小支撑树中。 
）
先任取一点组成左集合（右集合为点补集）找出与它相邻的最小权边。把最小权边的另一点加入左集合 

破圈法
去掉任意圈中最大的边， 一直进行
两个图之间最短的连接接 

印第安那州公路规划问题
树图与图的最小支撑树。 
最短路问题


两点间的最短路dijkstra

无向路最短路
最短路的子集也是最短的。 
求两个图之间的最小连接点 递推下去。 

有向图最短路
和dijkstra相似， 但不能逆标号

各点之间最短路（中心问题）乡政府设点问题 重心问题  消防站问题
最短路矩阵


用于解决最短路径问题的算法被称做“最短路径算法”，有时被简称作“路径算法”。最常用的路径算法有：

Dijkstra算法
A*算法
Bellman-Ford算法
SPFA算法（Bellman-Ford算法的改进版本）
Floyd-Warshall算法
Johnson算法
Bi-Direction BFS算法

Abdul Bari 讲解floyd warshall
4.2 All Pairs Shortest Path (Floyd-Warshall) - Dynamic Programming

All Pairs Shortest Path (Floyd-Warshall) - Dynamic Programming
多点最短路径动态规划
A^(k)[i, j] = min{A^(k-1)[i,j], A^(k-1)[i,k]+A^(k-1)[k,j]}
一遍遍优化矩阵。 递推完所有可能。 



线性规划
linearprogramming
整数线性规划， 变量只能取整数。 
integer linear programming
变量只能取0或1
二进制线性
binary linear programming. 




动态规划（ＤｙｎａｍｉｃＰｒｏｇｒａｍｍｉｎｇ）是现代企业管理中一种重要的决策方法，它是解决
多阶段决策过程最优化的一种数学方法，动态规划大约产生于２０世纪５０年代，１９５１年美
国数学家贝尔曼（Ｒ．Ｂｅｌｌｍａｎ）等人，根据一类多阶段决策问题的特点，把多阶段决策问题
转化为一系列相互联系的单阶段问题，然后逐个加以解决。同时，他提出了解决这类问题的
最优原理，研究了许多实际问题，从而创建了解决最优化问题的一种新的方法———动态规划
方法。动态规划方法，在工程技术、企业管理、军事等部门都有广泛的应用。特别在企业管
理中，动态规划方法可以用来解决最优路径问题、资源分配问题、生产调度问题、库存问
题、装载问题、排序问题、设备更新问题、生产过程最优控制问题等。动态规划是求解一类
问题的方法，是解决问题的一种原理，而不是一种特殊的算法，故需要有丰富的想像去建
模，创造性地去求解



线性代数 
构造域
构造加法  乘法  运算。 
可以把它当成数对， 可以把它看成力于方向， 也可以把它看成线性空间。



matrix  向量空间 是一个变换
当你将矩阵看做空间的变换之后， 此后几乎所有的主题
从矩阵乘法， 到行列式， 基变换，特征值等都会更加容易理解 。 
据我的经验， 如果丢掉矩阵的话， 那些涉及矩阵的证明可以缩短一半。 
it is my experience that proofs involving matrix can be shortened by 50% if one throws the matrices.
用矩阵描述线性变换。 


连续变换
composition of transform.
f(g(x))

good explanation > symbolic proof.


determinant of a transform.

变换对空间拉伸挤压的程度。 determinant.

对单位空间拉伸挤压的程度。
the determinant 判别式。 

In mathematics, the determinant is a scalar value that is a function of the entries of a square matrix.
dy/dx  dy在dx上的变化率


欧拉恒等式， 把指数和三解联系一起。 

PengTitus
PengTitus

那么问题来了，是否对于所有的 b，方程 Ax=b 都有解？
从列图像上看，问题转化为“列向量的线性组合是否覆盖整个三维空间？”


行空间  列空间

行列式  区域面积的缩放比例。 


三维空间是体积的缩放

平行六面体 parallelepeped



逆矩阵相等于逆变换。 



变换的rank 线性空间维度。 
秩变换后空间的维度。 

span of columns 
列张成的空间。  column space. 
full rank.

零空间的概念有助于我们理解所有可能的解的集合是什么样的。 


Physical meaning of the null space of a matrix：：：：
Let's suppose that the matrix A represents a physical system. As an example, let's assume our system is a rocket, and A is a matrix representing the directions we can go based on our thrusters. So what do the null space and the column space represent?

Well let's suppose we have a direction that we're interested in. Is it in our column space? If so, then we can move in that direction. The column space is the set of directions that we can achieve based on our thrusters. Let's suppose that we have three thrusters equally spaced around our rocket. If they're all perfectly functional then we can move in any direction. In this case our column space is the entire range. But what happens when a thruster breaks? Now we've only got two thrusters. Our linear system will have changed (the matrix A will be different), and our column space will be reduced.

What's the null space? The null space are the set of thruster intructions that completely waste fuel. They're the set of instructions where our thrusters will thrust, but the direction will not be changed at all.

Another example: Perhaps A can represent a rate of return on investments. The range are all the rates of return that are achievable. The null space are all the investments that can be made that wouldn't change the rate of return at all.

Another example: room illumination. The range of A represents the area of the room that can be illuminated. The null space of A represents the power we can apply to lamps that don't change the illumination in the room at all.
-------------------------------------------
-------------------------------------------
-------------------------------------------



在这个小测验里， 我让你们求一个2X3矩阵的行列式，让我感到非常可笑的是， 你们当中竟然有人尝试去做。 




非方阵  Nonsquare matrix 


非方阵注意列空间维数与输入空间相等


3X2矩阵3行2竖矩阵， 他是把二维映射到三维。 

2X3矩阵， 是把3维映射到2维。 
等距分布的点。 r相等。 


矩阵乘积和线性方程组的意义。 


点积 dot products.
点积与分向相关， 力的方向
等距分布的点 降维变换后还是等距的


对称 
叉积  通过两个三维生成一个新的三维。 
叉积指向负方向。 

英文学习方法， 你要说出来。 
多说多听


eigenvectors and eigenvalues



弗拉基米尔·阿诺德（英文Vladimir Igorevich Arnold，俄文Влади́мир И́горевич Арно́льд，1937~2010.6.3）
弗拉基米尔·阿诺德（英文Vladimir Igorevich Arnold，俄文Влади́мир И́горевич Арно́льд，1937~2010.6.3）



--------
线性规划
消防站问题
先动态规划， 找出所有的最短路。 
然后根据10分钟赶到作线性规划。 


中国邮递员问题。 
中国路员问题。 
抽象为图的语言， 就是给一个连通图， 在每边上赋予一个非负的权， 要求过每边至少一次， 并使图的总权最小。 



网络最大流问题
车辆流， 乘客流， 物资流， 资金流。 信息流。 

可行流
ford fulkerson方法。 
最小隔 最大流
增广链是关键。 

找出所有的增广链。 
Ford-Fulkerson Algorithm 
The following is simple idea of Ford-Fulkerson algorithm:
1) Start with initial flow as 0.
2) While there is a augmenting path from source to sink. 
           Add this path-flow to flow.
3) Return flow.




flow = 0
for each edge (u, v) in G:
    flow(u, v) = 0
while there is a path, p, from s -> t in residual network G_f:
    residual_capacity(p) = min(residual_capacity(u, v) : for (u, v) in p)
    flow = flow + residual_capacity(p)
    for each edge (u, v) in p:
        if (u, v) is a forward edge:
            flow(u, v) = flow(u, v) + residual_capacity(p)
        else:
            flow(u, v) = flow(u, v) - residual_capacity(p)
return flow

Augmenting paths
An augmenting path is a path (u1, u2, ..., uk) in the residual network, where u1 = s, uk = t, and cf (ui, ui + 1) > 0. A network is at maximum flow if and only if there is no augmenting path in the residual network Gf.
youtube youtube
Abdul Bari
航空公司的最大流量
也是用增广链方法



最小费用流问题

最小费用， 先在费用流找最短路， 然后在此路线相对的容量流， 找最大流， 不够再找另一条最小费用流， 然后在此路线相对的容量流， 找最大流， 
Minimum-cost flow problem (最小成本最大流問題)
Minimum cost flow problem
MCF问题
解法：最短路和最大流算法相结合。 
最短路只是起辅助作用。 增广链才是主要。 

还是动态规划的思想，分阶段优化，一遍遍优化。 
找出最短路， 找出对应增广链。 
然后那个流量满了应该不能做最短路了， 
最短路辅助图： 饱和弧只加反向弧， 零流弧只加正向弧， 其它弧是双向弧）
（饱和弧只加反向弧， 零流弧只加正向弧， 其它弧是双向弧）
为什么饱和弧只加反向（收费）弧：因为饱和了， 只能减流量， 不能加流量了。 
为什么零流弧只加正向弧： 零流弧， 没有流量， 没有可减的， 只能加流量。 
为什么其它弧是双向弧：因为流量也可增加也可减少， 所以得把两边的费用整好。 
费用最短路，只是为了辅助增广链。 



algothemn lecture.
#	Lecture Notes	Chapter(s)	In-class Demos
1	Introduction	1-5	 
2	Elementary sorts	6	Sorting applets
3	Mergesort, Quicksort	7, 8	Partitioning   Quicksort applets  
Merging   Mergesort applets
4	Priority queues	9	  Sort summary
5	Symbol tables	12	Growing Tree Tool
6	Balanced trees	13	Growing Tree Tool
7	Hashing	14	 
8	Radix sorts	10	3-way partitioning   C code
9	Trie searching	15	C code
10	String searching	19*	KMP demo   C code
11	Midterm
12	Pattern matching	20*, 21*	Deque simulation of NFSA
13	Data compression	22*	Huffman demo   LZW demo
14	Geometric algorithms	24*, 25*	Convex hull applet   Applications
15	Geometric search	26*, 27*	Voronoi applet
16	Undirected graphs	17, 18	DFS   BFS   Euler   Maze   C code
17	Minimum spanning tree	20	Graph applet   Applications
18	Directed graphs	19	DFS   Topological sort   C code
19	Shortest paths	21	Dijkstra demo
20	Max flow, min cut	22	Augmenting path demo   C code
21	Minimum cost flow	22	 
22	Linear programming	article	 
23	Reductions	-	 
24	All questions answered	-	 





中南大学运筹学
中南大学运筹学
夏伟怀
符卓中南大学 - 教授

中南大学运筹学
中南大学运筹学
夏伟怀
符卓中南大学 - 教授

决策者如何从多个决策上选出效益最大的（或损失最小的）决策。 



确定性决策问题， 风险型决策问题
最大概率准则。 
最大期望值准则。 
矩阵法
决策树法。 

效益*概率=期望收益
决策树法
把方案  状态 结果 状态概率 等用一棵树来表示 。 


点和边表征。 

风险型决策问题是一类已知状态概率的不确定型决策问题；
风险型决策的解决方法有矩阵法， 决策树法。 


不确定型问题：
解决方法：
1。等可能准则。假设各种自然状态出现的概率是一样的。 
2。乐观准则。每种方案取最有利的情况。 
3。悲观准则。
4。最小后悔值准则 max-min 
	1》确定各状态的理想目标， 并求后悔值 
	2》求各方案的最大后悔值，
	3》从最大后悔值中找最小的
	后悔值求法每种下找出最好值， 然后减所有的值。是在每一种状态下的后悔值。
5。折衷准则。

消费存贮问题  需求供应问题
inventory theory
供存销
存贮策略
1。t循环策略，每隔时间t补充存贮量Q。
2。小s大S策略。s最低库存量， S最大库存量
3。 t,s,S 混合策略。每隔一段时间，检查。 

费用标准来考虑存贮问题。
存贮费。 缺货费， 进货费。 
使费用最小， 使收益最大。 

确定型存储模型
eoq模型：瞬时进货， 不允许缺货。 
基于四个假设：
1。能立即补充
2。需求是均匀连续的，常数。
3。单位存贮费不变
4。订购费不变。
economic ordering quantity.


随机存贮问题
单周期随机存贮问题。
报童问题


排队论
随机服务系统理论
queuing theory:
无后效性

有平稳性， 无后效性 普通性的流 叫做poisson流
生灭过程及其状态平衡方程
排队论各项指标的计算
多通道排队模型。 

排队服务系统优化
系统设计优化静态优化
系统控制优化动态优化
MM1模型
愿望模型  愿望优化





顺序查找：for 循环
二分查找法，每次可能组的中间。 binarysearch 分而治之
binerysearch 适合递归算法。 
冒泡排序：不断两两对比交换， 大的往后放。 找出最大。 找出次大。 

选择排序： selection sort.　不每次交换位置， 只是通过对比找出最大的坐标。 最后再交换。 
插入排序：接牌一样。 手上的总是排好序。 最好每一次接的都比手上的大。 
shell sort 谢尔排序 
不需要大量的辅助空间，和归并排序一样容易实现。希尔排序是基于插入排序的一种算法， 在此算法基础之上增加了一个新的特性，提高了效率。希尔排序的时间的时间复杂度为O( )，希尔排序时间复杂度的下界是n*log2n。希尔排序没有快速排序算法快 O(n(logn))，因此中等大小规模表现良好，对规模非常大的数据排序不是最优选择。但是比O( )复杂度的算法快得多。并且希尔排序非常容易实现，算法代码短而简单。 此外，希尔算法在最坏的情况下和平均情况下执行效率相差不是很多，与此同时快速排序在最坏的情况下执行的效率会非常差。专家们提倡，几乎任何排序工作在开始时都可以用希尔排序，若在实际使用中证明它不够快，再改成快速排序这样更高级的排序算法. 本质上讲，希尔排序算法是直接插入排序算法的一种改进，减少了其复制的次数，速度要快很多。 原因是，当n值很大时数据项每一趟排序需要移动的个数很少，但数据项的距离很长。当n值减小时每一趟需要移动的数据增多，此时已经接近于它们排序后的最终位置。 正是这两种情况的结合才使希尔排序效率比插入排序高很多。Shell算法的性能与所选取的分组长度序列有很大关系。只对特定的待排序记录序列，可以准确地估算关键词的比较次数和对象移动次数。想要弄清关键词比较次数和记录移动次数与增量选择之间的关系，并给出完整的数学分析，今仍然是数学难题。


先分成大组， 进行插入排序， 
然后再分成小组， 
主要是想减少后边的交换。 


有点动态规划的意思 。 


Conceptually, a merge sort works as follows:

Divide the unsorted list into n sublists, each containing one element (a list of one element is considered sorted).
Repeatedly merge sublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list.



merge sort: 相比于快速排序和堆排序，归并算法的一个显著优点在于其稳定性，主要缺点在于需要线性的额外空间。

快速排序 
 function quicksort(q)
 {
     var list less, pivotList, greater
     if length(q) ≤ 1 
         return q
     else 
     {
         select a pivot value pivot from q
         for each x in q except the pivot element
         {
             if x < pivot then add x to less
             if x ≥ pivot then add x to greater
         }
         add pivot to pivotList
         return concatenate(quicksort(less), pivotList, quicksort(greater))
         }
 }


上面简单版本的缺点是，它需要{\displaystyle \Omega (n)}{\displaystyle \Omega (n)}的额外存储空间，也就跟归并排序一样不好。额外需要的存储器空间配置，在实际上的实现，也会极度影响速度和缓存的性能。有一个比较复杂使用原地（in-place）分割算法的版本，且在好的基准选择上，平均可以达到{\displaystyle O(\log n)}{\displaystyle O(\log n)}空间的使用复杂度。

 function partition(a, left, right, pivotIndex)
 {
     pivotValue = a[pivotIndex]
     swap(a[pivotIndex], a[right]) // 把pivot移到結尾
     storeIndex = left
     for i from left to right-1
     {
         if a[i] <= pivotValue
          {
             swap(a[storeIndex], a[i])
             storeIndex = storeIndex + 1
          }
     }
     swap(a[right], a[storeIndex]) // 把pivot移到它最後的地方
     return storeIndex
 }


 bublesort insertsort mergesort quicksort
 排序总体是为了减少重复的比较。 和移动。 
 mergesort神似Insertsort. 
 mergesort可以在概率上减少对比和移动。 但mergesort增加了空间要求。 
 quicksort比mergesort在概率上减少对比。 移动还没思考清楚。 


 思考每个算法都有什么缺陷 思考新算法是为了解决什么问题。 
 通过类比  比较   学习。 



 hashlib 
 sha256
线性探测  linear probing

线性探测法的一个缺点是有聚集性。 
散列冲突




o /bin Contains ready-to-run programs (also known as an executables), including most of the basic Unix 
commands such as ls and cp. Most of the programs in /bin are in binary format, having been created by a 
C compiler, but some are shell scripts in modern systems.
o /dev Contains device files. You’ll learn more about these in Chapter 3.
o /etc This core system configuration directory (pronounced EHT-see) contains the user password, boot, 
device, networking, and other setup files. Many items in /etc are specific to the machine’s hardware. For 
example, the /etc/X11 directory contains graphics card and window system configurations.
o /home Holds personal directories for regular users. Most Unix installations conform to this standard.
o /lib An abbreviation for library, this directory holds library files containing code that executables can use. 
There are two types of libraries: static and shared. The /lib directory should contain only shared libraries, 
but other lib directories, such as /usr/lib, contain both varieties as well as other auxiliary files. (We’ll 
discuss shared libraries in more detail in Chapter 15.)
o /proc Provides system statistics through a browsable directory-and-file interface. Much of the /proc
subdirectory structure on Linux is unique, but many other Unix variants have similar features. The /proc
directory contains information about currently running processes as well as some kernel parameters.
o /sys This directory is similar to /proc in that it provides a device and system interface. You’ll read more 
about /sys in Chapter 3.
o /sbin The place for system executables. Programs in /sbin directories relate to system management, so 
regular users usually do not have /sbin components in their command paths. Many of the utilities found 
here will not work if you’re not running them as root.


o /tmp A storage area for smaller, temporary files that you don’t care much about. Any user may read to and 
write from /tmp, but the user may not have permission to access another user’s files there. Many programs 
use this directory as a workspace. If something is extremely important, don’t put it in /tmp because most 
distributions clear /tmp when the machine boots and some even remove its old files periodically. Also, don’t 
let /tmp fill up with garbage because its space is usually shared with something critical (like the rest of /, for 
example).
o /usr Although pronounced “user,” this subdirectory has no user files. Instead, it contains a large directory 
hierarchy, including the bulk of the Linux system. Many of the directory names in /usr are the same as those 
in the root directory (like /usr/bin and /usr/lib), and they hold the same type of files. (The reason that the 
root directory does not contain the complete system is primarily historic—in the past, it was to keep space 
requirements low for the root.)
o /var The variable subdirectory, where programs record runtime information. System logging, user tracking, 
caches, and other files that system programs create and manage are here. (You’ll notice a /var/tmp directory 
here, but the system doesn’t wipe it on boot.)



2.19.1 Other Root Subdirectories
There are a few other interesting subdirectories in the root directory:
o /boot Contains kernel boot loader files. These files pertain only to the very first stage of the Linux startup 
procedure; you won’t find information about how Linux starts up its services in this directory. See 
Chapter 5 for more about this.
o /media A base attachment point for removable media such as flash drives that is found in many 
distributions.
o /opt This may contain additional third-party software. Many systems don’t use /opt.
2.19.2 The /usr Directory
The /usr directory may look relatively clean at first glance, but a quick look at /usr/bin and /usr/lib reveals 
that there’s a lot here; /usr is where most of the user-space programs and data reside. In addition to /usr/bin, 
/usr/sbin, and /usr/lib, /usr contains the following:
o /include Holds header files used by the C compiler.
o /info Contains GNU info manuals (see 2.13 Getting Online Help).
o /local Is where administrators can install their own software. Its structure should look like that of / and /usr.
o /man Contains manual pages.
o /share Contains files that should work on other kinds of Unix machines with no loss of functionality. In the 
past, networks of machines would share this directory, but a true /share directory is becoming rare because 
there are no space issues on modern disks. Maintaining a /share directory is often just a pain. In any case, 
/man, /info, and some other subdirectories are often found here.





2.20.1 sudo
Most larger distributions use a package called sudo to allow administrators to run commands as root when 
they are logged in as themselves. For example, in Chapter 7, you’ll learn about using vipw to edit the 
/etc/passwd file. You could do it like this:
$ sudo vipw
When you run this command, sudo logs this action with the syslog service under the local2 facility. You’ll 
also learn more about system logs in Chapter 7.
2.20.2 /etc/sudoers
Of course, the system doesn’t let just any user run commands as the superuser; you must configure the 
privileged users in your /etc/sudoers file. The sudo package has many options (that you’ll probably never 
use), which makes the syntax in /etc/sudoers somewhat complicated. For example, this file gives user1 and 
user2 the power to run any command as root without having to enter a password:
User_Alias ADMINS = user1, user2
ADMINS ALL = NOPASSWD: ALL
root ALL=(ALL) ALL
The first line defines an ADMINS user alias with the two users, and the second line grants the privileges. The 
ALL = NOPASSWD: ALL part means that the users in the ADMINS alias can use sudo to execute commands 
as root. The second ALL means “any command.” The first ALL means “any host.” (If you have more than one 
machine, you can set different kinds of access for each machine or group of machines, but we won’t cover 
that feature.)
The root ALL=(ALL) ALL simply means that the superuser may also use sudo to run any command on 
any host. The extra (ALL) means that the superuser may also run commands as any other user. You can 
extend this privilege to the ADMINS users by adding (ALL) to the /etc/sudoers line, as shown at ➊:
ADMINS ALL = (ALL)➊ NOPASSWD: ALL
NOTE
Use the visudo command to edit /etc/sudoers. This command checks for file syntax errors after 
you save the file.
www.it-ebooks.infoThat’s it for sudo for now. If you need to use its more advanced features, see the sudoers(5) and sudo(8) 
manual pages. (The actual mechanics of user switching are covered in Chapter 7.)





s does any command with redirected output, this sends some stuff from the standard output to a file. However, 
the file is /dev/null, a device, and the kernel decides what to do with any data written to this device. In the case 
of /dev/null, the kernel simply ignores the input and throws away the data.
To identify a device and view its permissions, use ls -l:
Example 3-1. Device files
$ ls -l
brw-rw---- 1 root disk 8, 1 Sep 6 08:37 sda1
crw-rw-rw- 1 root root 1, 3 Sep 6 08:37 null
prw-r--r-- 1 root root 0 Mar 3 19:17 fdata
srw-rw-rw- 1 root root 0 Dec 18 07:43 log
Note the first character of each line (the first character of the file’s mode) in Example 3-1. If this character is 
b, c, p, or s, the file is a device. These letters stand for block, character, pipe, and socket, respectively, as 
described in more detail below.





 dd and Devices
The program dd is extremely useful when working with block and character devices. This program’s sole 
function is to read from an input file or stream and write to an output file or stream, possibly doing some 
encoding conversion on the way.
dd copies data in blocks of a fixed size. Here’s how to use dd with a character device and some common 
options:
$ dd if=/dev/zero of=new_file bs=1024 count=1
As you can see, the dd option format differs from the option formats of most other Unix commands; it’s based 
on an old IBM Job Control Language (JCL) style. Rather than use the dash (-) character to signal an option, 
you name an option and set its value to something with the equals (=) sign. The preceding example copies a 
single 1024-byte block from /dev/zero (a continuous stream of zero bytes) to new_file.
These are the important dd options:
o if=file The input file. The default is the standard input.
o of=file The output file. The default is the standard output.
o bs=size The block size. dd reads and writes this many bytes of data at a time. To abbreviate large chunks 
of data, you can use b and k to signify 512 and 1024 bytes, respectively. Therefore, the example above 
could read bs=1k instead of bs=1024.


# chvt 1


udev

The rules files are in the /lib/udev/rules.d and /etc/udev/rules.d directories. T


udevadm monitor --kernel --subsystem-match=scsi
For more on udevadm, see the udevadm(8) manual page.
There’s much more to udev. For example, the D-Bus system for interprocess communication has a daemon 
called udisks-daemon that listens to the outgoing udevd events in order to automatically attach disks and 
to further notify other desktop software that a new di


There are many kinds of partition tables. The traditional table is the one found inside the Master Boot Record 
(MBR). A newer standard starting to gain traction is the Globally Unique Identifier Partition Table (GPT).
Here is an overview of the many Linux partitioning tools available:
o parted A text-based tool that supports both MBR and GPT.
o gparted A graphical version of parted.
o fdisk The traditional text-based Linux disk partitioning tool. fdisk does not support GPT.
o gdisk A version of fdisk that supports GPT but not MBR.
Because it supports both MBR and GPT, we’ll use parted in this book. However, many people prefer the 
fdisk interface, and there’s nothing wrong with that.




fsck /dev/sdb1
www.it-ebooks.infoWARNING
You should never use fsck on a mounted filesystem because the kernel may alter the disk data 
as you run the check, causing runtime mismatches that can crash your system and corrupt files. 
There is only one exception: If you mount the root partition read-only in single-user mode, you may 
use fsck on it.


cat /proc/cmdline
BOOT_IMAGE=/boot/vmlinuz-3.2.0-67-generic-pae root=UUID=70ccd6e7-6ae6-
44f6-
 812c-51aab8036d29 ro quiet splash vt.handoff=7
The parameters are either simple one-word flags, such as ro and quiet, or key=value pairs, such as 
vt.handoff=7. Many of the parameters are unimportant, such as the splash flag for displaying a splash 
screen, but one that is critical is the root parameter. This is the location of the root filesystem; without it, the 
kernel cannot find init and therefore cannot perform the user space start.
The root filesystem can be specified as a device file, such as in this example:
root=/dev/sda1
However, on most modern desktop systems, a UUID is more common (see 4.2.4 Filesystem UUID):
root=UUID=70ccd6e7-6ae6-44f6-812c-51aab8036d29
The ro parameter is normal; it instructs the kernel to mount the root filesys




 UEFI Boot
PC manufacturers and software companies realized that the traditional PC BIOS is severely limited, so they 
decided to develop a replacement called Extensible Firmware Interface (EFI). EFI took a while to catch on 
for most PCs, but now it’s fairly common. The current standard is Unified EFI (UEFI), which includes features 
such as a built-in shell and the ability to read partition tables and navigate filesystems. The GPT partitioning 
scheme is part of the UEFI standard.


User space starts in roughly this order:
1. init
2. Essential low-level services such as udevd and syslogd
3. Network configuration
4. Mid- and high-level services (cron, printing, and so on)
5. Login prompts, GUIs, and other high-level applications


There are three major implementations of init in Linux distributions:
o System V init. A traditional sequenced init (Sys V, usually pronounced “sys-five”). Red Hat Enterprise 
Linux and several other distributions use this version.
o systemd. The emerging standard for init. Many distributions have moved to systemd, and most that have 
not yet done so are planning to move to it.
o Upstart. The init on Ubuntu installations. However, as of this writing, Ubuntu has also planned to migrate 
to systemd.


A multiuser system must provide basic support for user security in terms of identification and authentication. 
The identification portion of security answers the question of who users are. The authentication piece asks 
users to prove that they are who they say they are. Finally, authorization is used to define and limit what users 
are allowed to do.





Control:
Most systems have at least 4 control bus connections (active low).
MRDC (Memory ReaD Control), MWRC , IORC (I/O Read Control), IOWC


Original bus of the IBM PC.
Includes the following signals (not a complete list)
the processor clock
power and ground
20 address lines
8 data lines
memory read, memory write, IO read, IO write lines (set by processor)


Control and Status Registers
Each I/O device is connected to the I/O bus through a controller. A simple controller will have at least 3 addresses (ports) on the bus, each corresponding to a register in the controller
a data register (either readable or writable, depending on whether it is an input or output device)
a control register (writable, for controlling device operation)
a status register (readable, for determining device status -- in particular, whether it is ready to receive or provide data)
More complex devices (e.g., disks) will have multiple control and status registers


The Memory Manager portion of the OS:
Tracks memory usage.
Allocates/Deallocates memory.
Implements virtual memory.


Simple Memory Management：
Advantage: it's simple to implement.
However, it utilizes memory poorly. Also, in time sharing systems, queueing up jobs in this manner leads to unacceptable response time for user processes.


分段  分页
Two concepts:
Segmentation: Allows the OS to "share" code and enforce meaningful constraints on the memory used by a process, e.g. no execution of data.
Paging: Allows the OS to efficiently manage physical memory, and makes it easier to implement virtual memory.



Size :
The Pentium uses 32-bit virtual addresses.
With a 4K page size, a 32-bit address space has 2 32 /2 12 = 2 20 or 1,048,576 virtual page numbers !
If each page table entry occupies 4 bytes, that's 4MB of memory, just to store the page table.
 


Page Table Design Alternatives
Single page table stored in an array of fast hardware registers.
OS loads registers from memory when a process is started.
Advantage: No memory references are needed for the page table.
Disadvantage: Context switches require the entire page table to be loaded.
If it is large, this will be expensive.
 

Page table kept entirely in main memory.
Single register points to the start of the page table.
Advantage: Context switches only require updating the register pointer.
Disadvantage: One or more memory references are needed to read page table entries for each instruction.
 

Modern computers keep "frequently used" page table entries on chip in a cache (similar to first alternative above) and the others in main memory (similar to the second alternative).




Memory Paging:
The page directory is 4K bytes.
Each page table is 4K bytes, and there are 1024 of them.
If all 4GB of memory is paged, the overhead is 4MB!



The current scheme requires three accesses to memory:
One to the directory , one to the appropriate page table and (finally) one to the desired data or code item. Ouch!
A Translation Look-aside Buffer ( TLB ) is used to cache page directory and page table entries to reduce the number of memory references.
Plus the data cache is used to hold recently accessed memory blocks.
System performance would be extremely bad without these features.
Much more on this in OS (CMSC 421).


8086/88 Pinout
Pin functions:
AD15-AD0
Multiplexed address(ALE=1)/data bus(ALE=0).
A19/S6-A16/S3 (multiplexed)
High order 4 bits of the 20-bit address OR status bits S6-S3.
M/IO
Indicates if address is a Memory or IO address.
RD
When 0, data bus is driven by memory or an I/O device.
WR
Microprocessor is driving data bus to memory or an I/O device. When 0, data bus contains valid data.
ALE (Address latch enable)
When 1, address data bus contains a memory or I/O address.
DT/R (Data Transmit/Receive)
Data bus is transmitting/receiving data.
DEN (Data bus Enable)
Activates external data bus buffers.


8086/88 Pinout
Pin functions:
S7, S6, S5, S4, S3, S2, S1, S0
S7: Logic 1, S6: Logic 0.
S5: Indicates condition of IF flag bits.
S4-S3: Indicate which segment is accessed during current bus cycle:

S2, S1, S0 : Indicate function of current bus cycle (decoded by 8288).



The taxonomy of the descriptors is

Descriptors
    Non system descriptors

        Code segment descriptor
        Data segment descriptor
        Stack segment descriptor (Alias of the previous)

    System descriptors

        System segment descriptors

            LDT segment descriptor
            TSS segment descriptor

        Gate descriptors
            Call gate descriptor
            Interrupt gate descriptor
            Trap gate descriptor
            Task gate descriptor




xor eax, eax     ;eax is zero
xor esp, esp     ;esp is zero
xor ebx, ebx     ;ebx is zero

mov ecx, DWORD [eax]      ;Use DS selector (implicit)
mov ecx, DWORD [esp]      ;Use SS selector (implicit)
mov ecx, DWORD [fs:ebx]   ;Use FS selector (explicit)
All these instructions read the logical address 0 but the CPU uses the descriptor to compute a new address, called linear address and perform security checks.
So those three instructions may end up reading totally different addresses.



DPL: descriptor privilege level


Each selector also specifies the privilege which should be used when carrying out an operation.
The cs selector is special because it cannot be longer changed (it has been a while actually) with a mov but only with a branch instruction (jmp, ret, call, ...).
Its purpose is not only to be used when fetching code, it also holds the code privilege level.
This privilege level is used by the CPU to check if a resource can be accessed (with the requested privilege), checks are not always trivial.

As you will see, every descriptor has a DPL field to set its privilege level.
So they are a form of protection.

Non system descriptors
Non system descriptors are used to define regions of memory intended to store code or data along with their attributes.


As you can see, the purpose of this kind of descriptors is to designate an area of memory and attach some attributes to it.
In particular the base address, the limit (size), the privilege needed to access it (the DPL field, checks are actually more involved than this), the size of the code (code only), if read/write is allowed and so on.

Long mode (64 bits) changed how the attributes are interpreted, beware of that.


System descriptors
System descriptors are used by the OS to control user mode programs.

System segment descriptors
These descriptors define the memory area used for storing the LDT and another structure called Task State Segment (a mechanism Intel provided for easing task switching).
There can be more than one of these structures on the system, the selected ones are indicated by the ldtr (LDT register) and tsr (TS register) registers.


Gate descriptors
These are used to transfer control to other (more or less privileged) code.

Call gates



Gate descriptors
These are used to transfer control to other (more or less privileged) code.

Call gates

Call gate

If you look at the picture you can see that a call gate is essentially a meta descriptor, it specifies a selector and an offset into the area designated by that descriptor along with privileges.
It is used to pass control to privileged routines.

call fs:0badbabeh
Assuming fs holds a gate's index, the CPU won't use the immediate address 0badbabeh at all, it will instead use the information on the gate itself.

Interrupt and Trap gates

These are used with interrupts, the difference between the two is that the former clear the if flag, the latter doesn't.

They are very similar to call gates.

Interrupt and Trap gates

These descriptors are actually placed into another table, the Interrupt Descriptor Table usually.
This other table is not indexed with selectors but with interrupt numbers.
If I recall correctly they can also be placed in the GDT/LDT and used like other gates.

A Task gate can be used to perform a task switch.

Task gates

Task gates

These are like the call gates but transfer control to a new task (task switching).

Some resources are not simple memory area, they can be gates


7.5 Task Switching
The 80386 switches execution to another task in any of four cases:
The current task executes a JMP or CALL that refers to a TSS descriptor.
The current task executes a JMP or CALL that refers to a task gate.
An interrupt or exception vectors to a task gate in the IDT.
The current task executes an IRET when the NT flag is set.


Returns program control from an exception or interrupt handler to a program or procedure that was interrupted by an exception, an external interrupt, or a software-generated interrupt. These instructions are also used to perform a return from a nested task. (A nested task is created when a CALL instruction is used to initiate a task switch or when an interrupt or exception causes a task switch to an interrupt or exception handler.) See the section titled “Task Linking” in Chapter 7 of the Intel® 64 and IA-32 Architectures Software Developer’s Manual, Volume 3A.




A task switching operation involves these steps:

Checking that the current task is allowed to switch to the designated task. Data-access privilege rules apply in the case of JMP or CALL instructions. The DPL of the TSS descriptor or task gate must be numerically greater (e.g., lower privilege level) than or equal to the maximum of CPL and the RPL of the gate selector. Exceptions, interrupts, and IRET are permitted to switch tasks regardless of the DPL of the target task gate or TSS descriptor.
Checking that the TSS descriptor of the new task is marked present and has a valid limit. Any errors up to this point occur in the context of the outgoing task. Errors are restartable and can be handled in a way that is transparent to applications procedures.
Saving the state of the current task. The processor finds the base address of the current TSS cached in the task register. It copies the registers into the current TSS (EAX, ECX, EDX, EBX, ESP, EBP, ESI, EDI, ES, CS, SS, DS, FS, GS, and the flag register). The EIP field of the TSS points to the instruction after the one that caused the task switch.
Loading the task register with the selector of the incoming task's TSS descriptor, marking the incoming task's TSS descriptor as busy, and setting the TS (task switched) bit of the MSW. The selector is either the operand of a control transfer instruction or is taken from a task gate.
Loading the incoming task's state from its TSS and resuming execution. The registers loaded are the LDT register; the flag register; the general registers EIP, EAX, ECX, EDX, EBX, ESP, EBP, ESI, EDI; the segment registers ES, CS, SS, DS, FS, and GS; and PDBR. Any errors detected in this step occur in the context of the incoming task. To an exception handler, it appears that the first instruction of the new task has not yet executed.




Chapter 9 Exceptions and Interrupts
Interrupts and exceptions are special kinds of control transfer; they work somewhat like unprogrammed CALLs. They alter the normal program flow to handle external events or to report errors or exceptional conditions. The difference between interrupts and exceptions is that interrupts are used to handle asynchronous events external to the processor, but exceptions handle conditions detected by the processor itself in the course of executing instructions.

There are two sources for external interrupts and two sources for exceptions:

Interrupts
Maskable interrupts, which are signalled via the INTR pin.
Nonmaskable interrupts, which are signalled via the NMI (Non-Maskable Interrupt) pin.
Exceptions
Processor detected. These are further classified as faults, traps, and aborts.
Programmed. The instructions INTO, INT 3, INT n, and BOUND can trigger exceptions. These instructions are often called "software interrupts", but the processor handles them as exceptions.
This chapter explains the features that the 80386 offers for controlling and respondin




program as tree.
--------
#lang plai-typed
(define-type ArithC
[numC (n : number)]
[plusC (l : ArithC) (r : ArithC)]
[multC (l : ArithC) (r : ArithC)])


(define (parse [s : s-expression]) : ArithC
(cond
[(s-exp-number? s) (numC (s-exp->number s))]
[(s-exp-list? s)
(let ([sl (s-exp->list s)])
(case (s-exp->symbol (first sl))
[(+) (plusC (parse (second sl)) (parse (third sl)))]
[(*) (multC (parse (second sl)) (parse (third sl)))]
[else (error 'parse "invalid list input")]))]
[else (error 'parse "invalid input")]))

(parse '(+ 1 3))


--------


We might want to display a program in an attractive way
(“pretty-print”), convert into code in some other format (“compilation”), ask whether
it obeys certain properties (“verification”), and so on. For now, we’re going to focus
on asking what value it corresponds to (“evaluation”—the reduction of programs to
values).


your first interpreter
-----------------------------------------------------------
#lang plai-typed
(define-type ArithC
[numC (n : number)]
[plusC (l : ArithC) (r : ArithC)]
[multC (l : ArithC) (r : ArithC)])


(define (parse [s : s-expression]) : ArithC
(cond
[(s-exp-number? s) (numC (s-exp->number s))]
[(s-exp-list? s)
(let ([sl (s-exp->list s)])
(case (s-exp->symbol (first sl))
[(+) (plusC (parse (second sl)) (parse (third sl)))]
[(*) (multC (parse (second sl)) (parse (third sl)))]
[else (error 'parse "invalid list input")]))]
[else (error 'parse "invalid input")]))

(parse '(+ 1 3))

(define (interp [a : ArithC]) : number
(type-case ArithC a
[numC (n) n]
[plusC (l r) (+ (interp l) (interp r))]
[multC (l r) (* (interp l) (interp r))]))


(define a (parse '(+ 1 3)))
(interp a)

-----------------------------------------------------------


. Semantics is the mapping of syntax (e.g.,
+) to meaning (e.g., some or all of the above)




#lang plai-typed

(require (typed-in racket/base (raise-user-error : (string -> void))))

(define-type FieldP
  [fieldP (name : string) (value : ExprP)])

(define-type LHS
  [BracketLHS (obj : ExprP) (field : ExprP)]
  [DotLHS (obj : ExprP) (field : symbol)]
  [IdLHS (id : symbol)])

;; ExprPs are ParselTongue's toplevel:
(define-type ExprP
  [ObjectP (fields : (listof FieldP))]
  [DotP (obj : ExprP) (field : symbol)]
  [BracketP (obj : ExprP) (field : ExprP)]
  [DotMethodP (obj : ExprP) (field : symbol) (args : (listof ExprP))]
  [BrackMethodP (obj : ExprP) (field : ExprP) (args : (listof ExprP))]

  [FuncP (args : (listof symbol)) (body : ExprP)]
  [AppP (func : ExprP) (args : (listof ExprP))]
  [DefvarP (id : symbol) (bind : ExprP) (body : ExprP)]
  [DeffunP (name : symbol) (ids : (listof symbol)) (funbody : ExprP) (body : ExprP)]
  [IdP (name : symbol)]

  [WhileP (test : ExprP) (body : ExprP)]
  [ForP (init : ExprP) (test : ExprP) (update : ExprP) (body : ExprP)]

  [AssignP (lhs : LHS) (value : ExprP)]

  [SeqP (es : (listof ExprP))]
  [IfP (cond : ExprP) (then : ExprP) (else : ExprP)]

  [NumP (n : number)]
  [StrP (s : string)]
  [TrueP]
  [FalseP]

; An op is one of '+ '- '== 'print
  [PrimP (op : symbol) (args : (listof ExprP))]
; A PrimAssign op is one of '+ '-
  [PrimAssignP (op : symbol) (lhs : LHS) (value : ExprP)]

  [PreIncP (lhs : symbol)]
  [PostIncP (lhs : symbol)]
  [PreDecP (lhs : symbol)]
  [PostDecP (lhs : symbol)])

(define-type FieldC
  [fieldC (name : string) (value : ExprC)])

(define-type ExprC

  [ObjectC (fields : (listof FieldC))]
  [GetFieldC (obj : ExprC) (field : ExprC)]
  [SetFieldC (obj : ExprC) (field : ExprC) (value : ExprC)]

  [FuncC (args : (listof symbol)) (body : ExprC)]
  [AppC (func : ExprC) (args : (listof ExprC))]
  [LetC (id : symbol) (bind : ExprC) (body : ExprC)]
  [IdC (id : symbol)]
  [Set!C (id : symbol) (value : ExprC)]

  [IfC (cond : ExprC) (then : ExprC) (else : ExprC)]
  [SeqC (e1 : ExprC) (e2 : ExprC)]

  [NumC (n : number)]
  [StrC (s : string)]
  [TrueC]
  [FalseC]

  [ErrorC (expr : ExprC)]

; The core operations are 'string+ 'num+ 'num- '== '< '> 'print 'tagof
  [Prim1C (op : symbol) (arg : ExprC)]
  [Prim2C (op : symbol) (arg1 : ExprC) (arg2 : ExprC)])

(define-type-alias Env (listof Binding))
(define-type Binding
  [bind (name : symbol) (value : Location)])

(define-type-alias Location number)

(define-type FieldV
  [fieldV (name : string) (value : ValueC)])

(define-type ValueC
  [ObjectV (fields : (listof FieldV))]
  [ClosureV (args : (listof symbol)) (body : ExprC) (env : Env)]
  [NumV (n : number)]
  [StrV (s : string)]
  [TrueV]
  [FalseV])

(define (pretty-value (v : ValueC)) : string
  (type-case ValueC v
    [ObjectV (fs) "object"]
    [ClosureV (a b e) "function"]
    [NumV (n) (to-string n)]
    [StrV (s) s]
    [TrueV () "true"]
    [FalseV () "false"]))

(define (interp-error str)
  (begin
    (raise-user-error str)
    (error 'interp str)))






#lang plai-typed

(define-type FieldP
  [fieldP (name : string) (value : ExprP)])

(define-type LHS
  [BracketLHS (obj : ExprP) (field : ExprP)]
  [DotLHS (obj : ExprP) (field : symbol)]
  [IdLHS (id : symbol)])

;; ExprPs are ParselTongue's toplevel:
(define-type ExprP
  [ObjectP (fields : (listof FieldP))]
  [DotP (obj : ExprP) (field : symbol)]
  [BracketP (obj : ExprP) (field : ExprP)]
  [DotMethodP (obj : ExprP) (field : symbol) (args : (listof ExprP))]
  [BrackMethodP (obj : ExprP) (field : ExprP) (args : (listof ExprP))]

  [FuncP (args : (listof symbol)) (body : ExprP)]
  [AppP (func : ExprP) (args : (listof ExprP))]
  [DefvarP (id : symbol) (bind : ExprP) (body : ExprP)]
  [DeffunP (name : symbol) (ids : (listof symbol)) (funbody : ExprP) (body : ExprP)]
  [IdP (name : symbol)]

  [WhileP (test : ExprP) (body : ExprP)]
  [ForP (init : ExprP) (test : ExprP) (update : ExprP) (body : ExprP)]

  [AssignP (lhs : LHS) (value : ExprP)]

  [SeqP (es : (listof ExprP))]
  [IfP (cond : ExprP) (then : ExprP) (else : ExprP)]

  [NumP (n : number)]
  [StrP (s : string)]
  [TrueP]
  [FalseP]

; An op is one of '+ '- '== 'print '< '>
  [PrimP (op : symbol) (args : (listof ExprP))]
; A PrimAssign op is one of '+ '-
  [PrimAssignP (op : symbol) (lhs : LHS) (value : ExprP)]

  [PreIncP (lhs : symbol)]
  [PostIncP (lhs : symbol)]
  [PreDecP (lhs : symbol)]
  [PostDecP (lhs : symbol)])

(define-type FieldC
  [fieldC (name : string) (value : ExprC)])

(define-type ExprC

  [ObjectC (fields : (listof FieldC))]
  [GetFieldC (obj : ExprC) (field : ExprC)]
  [SetFieldC (obj : ExprC) (field : ExprC) (value : ExprC)]

  [FuncC (args : (listof symbol)) (body : ExprC)]
  [AppC (func : ExprC) (args : (listof ExprC))]
  [LetC (id : symbol) (bind : ExprC) (body : ExprC)]
  [IdC (id : symbol)]
  [Set!C (id : symbol) (value : ExprC)]

  [IfC (cond : ExprC) (then : ExprC) (else : ExprC)]
  [SeqC (e1 : ExprC) (e2 : ExprC)]

  [NumC (n : number)]
  [StrC (s : string)]
  [TrueC]
  [FalseC]

  [ErrorC (expr : ExprC)]

; The core operations are 'string+ 'num+ 'num- '== '< '> 'print 'tagof
  [Prim1C (op : symbol) (arg : ExprC)]
  [Prim2C (op : symbol) (arg1 : ExprC) (arg2 : ExprC)])
